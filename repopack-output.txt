This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-02-25T22:05:23.663Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
config/settings.py
main.py
models/gradient_boosting.py
models/model_factory.py
models/polynomial_regression.py
models/random_forest.py
models/svr_model.py
pages/1_Data_Upload.py
pages/2_Data_Analysis.py
pages/3_Model_Training.py
pages/4_Predictions.py
pages/5_Model_Comparison.py
pages/6_About.py
utils/data_processor.py
utils/model_comparison.py
utils/model_management_ui.py
utils/model_manager.py
utils/visualizations.py

================================================================
Repository Files
================================================================

================
File: config/settings.py
================
# App settings
APP_NAME = "Poultry Weight Predictor"
APP_ICON = "🐔"
LAYOUT = "wide"

# Data settings
REQUIRED_COLUMNS = [
    'Int Temp',
    'Int Humidity',
    'Air Temp',
    'Wind Speed',
    'Feed Intake',
    'Weight'
]

FEATURE_COLUMNS = [
    'Int Temp',
    'Int Humidity',
    'Air Temp',
    'Wind Speed',
    'Feed Intake'
]

TARGET_COLUMN = 'Weight'

# Model settings
TEST_SIZE = 0.2
RANDOM_STATE = 42
POLYNOMIAL_DEGREE = 2

# File paths
MODEL_SAVE_PATH = "models/saved_models"
TEMP_DATA_PATH = "temp/data"

# Visualization settings
THEME_COLORS = {
    'primary': '#FF4B4B',
    'secondary': '#0083B8',
    'background': '#FFFFFF',
    'text': '#262730'
}

PLOT_HEIGHT = 500
PLOT_WIDTH = 800

================
File: main.py
================
import streamlit as st
from config.settings import APP_NAME, APP_ICON, LAYOUT

# Configure the Streamlit page
st.set_page_config(
    page_title=APP_NAME,
    page_icon=APP_ICON,
    layout=LAYOUT
)

# Main page header
st.title("🐔 Poultry Weight Predictor")

# Welcome message
st.markdown("""
## Welcome to the Poultry Weight Predictor

This application helps you predict poultry weight based on environmental and feeding data. 
You can:

1. Upload and analyze your poultry data
2. Train machine learning models
3. Make predictions on new data
4. Visualize results and insights

### Getting Started

Use the sidebar to navigate through different sections of the app:

- **Data Upload**: Upload and preview your data
- **Data Analysis**: Explore your data with visualizations
- **Model Training**: Train and evaluate prediction models
- **Predictions**: Make predictions on new data

### Required Data Format

Your CSV file should include the following columns:
- Internal Temperature (Int Temp)
- Internal Humidity (Int Humidity)
- Air Temperature (Air Temp)
- Wind Speed
- Feed Intake
- Weight (target variable)
""")

# Footer
st.markdown("---")
st.markdown("Built with ❤️ using Streamlit")

================
File: models/gradient_boosting.py
================
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.base import BaseEstimator, RegressorMixin
import numpy as np
import pandas as pd
import joblib
import os
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime

class PoultryGBRegressor(BaseEstimator, RegressorMixin):
    """
    Gradient Boosting Regressor for poultry weight prediction with enhanced functionality.
    Includes early stopping, feature importance analysis, and model persistence.
    Implements scikit-learn's estimator interface.
    """
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, 
                 min_samples_split=2, min_samples_leaf=1, subsample=1.0,
                 random_state=42, early_stopping_rounds=None, validation_fraction=0.1):
        """
        Initialize the Gradient Boosting model.
        
        Args:
            n_estimators: Number of boosting stages
            learning_rate: Learning rate shrinks the contribution of each tree
            max_depth: Maximum depth of the individual trees
            min_samples_split: Minimum samples required to split an internal node
            min_samples_leaf: Minimum samples required to be at a leaf node
            subsample: Fraction of samples to be used for fitting the individual trees
            random_state: Random state for reproducibility
            early_stopping_rounds: Number of rounds with no improvement to stop training
            validation_fraction: Fraction of training data to use for validation
        """
        # Convert parameters to correct types
        self.n_estimators = int(n_estimators)
        self.learning_rate = float(learning_rate)
        self.max_depth = int(max_depth)
        self.min_samples_split = int(min_samples_split)
        self.min_samples_leaf = int(min_samples_leaf)
        self.subsample = float(subsample)
        self.random_state = int(random_state)
        self.early_stopping_rounds = int(early_stopping_rounds) if early_stopping_rounds is not None else None
        self.validation_fraction = float(validation_fraction)
        
        # Initialize state variables
        self.model = None
        self._is_trained = False
        self.feature_names_ = None
        self._feature_importances = None
        self.training_metadata = {}

    def _get_model_params(self) -> Dict:
            """Get parameters for scikit-learn model."""
            return {
                'n_estimators': int(self.n_estimators),  # Ensure int
                'learning_rate': float(self.learning_rate),
                'max_depth': int(self.max_depth),
                'min_samples_split': int(self.min_samples_split),
                'min_samples_leaf': int(self.min_samples_leaf),
                'subsample': float(self.subsample),
                'random_state': int(self.random_state)
            }


    def _validate_input_data(self, X: np.ndarray, y: Optional[np.ndarray] = None, is_training: bool = True):
        """
        Validate input data for training or prediction.
        
        Args:
            X: Input features
            y: Target values (optional)
            is_training: Whether this is for training data
        """
        if X is None:
            raise ValueError("Input features cannot be None")
        if len(X) == 0:
            raise ValueError("Input features cannot be empty")
        if is_training:
            if y is None:
                raise ValueError("Target values cannot be None for training")
            if len(X) != len(y):
                raise ValueError("Number of samples in features and target must match")
            if len(X) < 2:
                raise ValueError("Need at least 2 samples for training")

    def fit(self, X, y):
        """
        Fit the model (scikit-learn interface).
        
        Args:
            X: Training features
            y: Target values
            
        Returns:
            self: The trained model instance
        """
        return self.train(X, y)

    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
              feature_names: Optional[List[str]] = None) -> 'PoultryGBRegressor':
        """
        Train the model with optional early stopping.
        
        Args:
            X_train: Training features
            y_train: Training targets
            feature_names: Optional list of feature names
        """
        try:
            # Validate input data
            self._validate_input_data(X_train, y_train, is_training=True)
            
            # Convert to numpy arrays
            X_train = np.asarray(X_train)
            y_train = np.asarray(y_train)
            
            # Store feature names if provided
            if feature_names is not None:
                self.feature_names_ = feature_names
            
            print(f"Training Gradient Boosting model with {len(X_train)} samples...")
            
            # Implementation of early stopping if enabled
            if self.early_stopping_rounds is not None:
                self._train_with_early_stopping(X_train, y_train)
            else:
                # Initialize base model with explicit parameters
                self.model = GradientBoostingRegressor(
                    n_estimators=int(self.n_estimators),
                    learning_rate=float(self.learning_rate),
                    max_depth=int(self.max_depth),
                    min_samples_split=int(self.min_samples_split),
                    min_samples_leaf=int(self.min_samples_leaf),
                    subsample=float(self.subsample),
                    random_state=int(self.random_state)
                )
                self.model.fit(X_train, y_train)
            
            # Store feature importances and mark as trained
            self._feature_importances = self.model.feature_importances_
            self._is_trained = True
            
            # Store training metadata
            self.training_metadata = {
                'n_samples': len(X_train),
                'n_features': X_train.shape[1],
                'training_date': datetime.now().isoformat(),
                'parameters': self._get_model_params(),
                'early_stopping_used': self.early_stopping_rounds is not None
            }
            
            print("Model training completed successfully")
            return self
            
        except Exception as e:
            print(f"Error during training: {str(e)}")
            raise




    def _train_with_early_stopping(self, X_train: np.ndarray, y_train: np.ndarray):
            """
            Implement early stopping training procedure.
            
            Args:
                X_train: Training features
                y_train: Training targets
            """
            try:
                # Split data for validation
                n_samples = len(X_train)
                n_val = int(n_samples * self.validation_fraction)
                indices = np.random.permutation(n_samples)
                val_indices = indices[:n_val]
                train_indices = indices[n_val:]
                
                # Split the data
                X_train_sub = X_train[train_indices]
                y_train_sub = y_train[train_indices]
                X_val = X_train[val_indices]
                y_val = y_train[val_indices]
                
                best_val_score = float('-inf')
                best_model = None
                patience_counter = 0
                
                print(f"Starting early stopping training with {len(X_train_sub)} training samples and {len(X_val)} validation samples")
                
                # Get base parameters and remove non-GradientBoostingRegressor parameters
                base_params = self._get_model_params()
                
                for n_est in range(1, self.n_estimators + 1):
                    # Create model with current number of estimators
                    current_model = GradientBoostingRegressor(
                        n_estimators=n_est,
                        learning_rate=self.learning_rate,
                        max_depth=self.max_depth,
                        min_samples_split=self.min_samples_split,
                        min_samples_leaf=self.min_samples_leaf,
                        subsample=self.subsample,
                        random_state=self.random_state
                    )
                    
                    # Train the model
                    current_model.fit(X_train_sub, y_train_sub)
                    
                    # Evaluate on validation set
                    val_score = current_model.score(X_val, y_val)
                    
                    # Print progress every 10 iterations
                    if n_est % 10 == 0:
                        print(f"Iteration {n_est}: validation score = {val_score:.4f}")
                    
                    if val_score > best_val_score:
                        best_val_score = val_score
                        best_model = current_model
                        patience_counter = 0
                        print(f"New best validation score: {best_val_score:.4f} at iteration {n_est}")
                    else:
                        patience_counter += 1
                        
                    if patience_counter >= self.early_stopping_rounds:
                        print(f"Early stopping triggered at iteration {n_est} (no improvement for {self.early_stopping_rounds} rounds)")
                        break
                
                if best_model is None:
                    raise ValueError("Training failed to produce a valid model")
                    
                self.model = best_model
                print(f"Training completed. Best validation score: {best_val_score:.4f}")
                
            except Exception as e:
                print(f"Error in early stopping training: {str(e)}")
                raise


    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions using the trained model.
        
        Args:
            X: Input features
            
        Returns:
            np.ndarray: Predicted values
        """
        if not self._is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        self._validate_input_data(X, is_training=False)
        return self.model.predict(X)

    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Tuple[Dict[str, float], np.ndarray]:
        """
        Evaluate model performance with multiple metrics.
        
        Args:
            X_test: Test features
            y_test: True test values
            
        Returns:
            Tuple containing:
                - Dictionary of evaluation metrics
                - Array of predicted values
        """
        if not self._is_trained:
            raise ValueError("Model must be trained before evaluation")
        
        try:
            self._validate_input_data(X_test, y_test, is_training=False)
            y_pred = self.predict(X_test)
            
            metrics = {
                'mse': mean_squared_error(y_test, y_pred),
                'rmse': mean_squared_error(y_test, y_pred, squared=False),
                'r2': r2_score(y_test, y_pred),
                'mae': mean_absolute_error(y_test, y_pred),
                'mape': np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            }
            
            return metrics, y_pred
            
        except Exception as e:
            print(f"Error during evaluation: {str(e)}")
            raise

    def get_feature_importance(self, feature_names: Optional[List[str]] = None) -> Dict[str, float]:
        """
        Get feature importance scores.
        
        Args:
            feature_names: Optional list of feature names to use
            
        Returns:
            Dict[str, float]: Dictionary mapping feature names to importance scores
        """
        if not self._is_trained:
            raise ValueError("Model must be trained before getting feature importance")
        
        try:
            # Use provided feature names, stored names, or generate default names
            if feature_names is None:
                feature_names = self.feature_names_
            if feature_names is None:
                feature_names = [f'feature_{i}' for i in range(len(self._feature_importances))]
            
            # Create and sort importance dictionary
            importance_dict = dict(zip(feature_names, self._feature_importances))
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
            
        except Exception as e:
            print(f"Error getting feature importance: {str(e)}")
            raise

    def save(self, filepath: str):
        """
        Save the trained model to a file.
        
        Args:
            filepath: Path where to save the model
        """
        if not self._is_trained:
            raise ValueError("Model must be trained before saving")
        
        try:
            save_dict = {
                'model': self.model,
                'params': self.get_params(),
                'feature_names': self.feature_names_,
                'feature_importances': self._feature_importances,
                'is_trained': self._is_trained,
                'training_metadata': self.training_metadata,
                'save_timestamp': datetime.now().isoformat()
            }
            
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            joblib.dump(save_dict, filepath)
            print(f"Model saved successfully to {filepath}")
            
        except Exception as e:
            print(f"Error saving model: {str(e)}")
            raise

    @classmethod
    def load(cls, filepath: str) -> 'PoultryGBRegressor':
        """
        Load a saved model from a file.
        
        Args:
            filepath: Path to the saved model file
            
        Returns:
            PoultryGBRegressor: Loaded model instance
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")
        
        try:
            save_dict = joblib.load(filepath)
            
            # Create new instance with saved parameters
            instance = cls(**save_dict['params'])
            
            # Restore saved state
            instance.model = save_dict['model']
            instance.feature_names_ = save_dict['feature_names']
            instance._feature_importances = save_dict['feature_importances']
            instance._is_trained = save_dict['is_trained']
            instance.training_metadata = save_dict['training_metadata']
            
            return instance
            
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            raise

    @property
    def feature_importances_(self):
        """Get feature importances (scikit-learn interface)."""
        if self.model is None:
            raise ValueError("Model not trained yet")
        return self._feature_importances

    @property
    def is_trained(self) -> bool:
        """Check if the model has been trained."""
        return self._is_trained

    def get_params(self, deep=True) -> Dict:
        """Get parameters (scikit-learn interface)."""
        return {
            'n_estimators': self.n_estimators,
            'learning_rate': self.learning_rate,
            'max_depth': self.max_depth,
            'min_samples_split': self.min_samples_split,
            'min_samples_leaf': self.min_samples_leaf,
            'subsample': self.subsample,
            'random_state': self.random_state,
            'early_stopping_rounds': self.early_stopping_rounds,
            'validation_fraction': self.validation_fraction
        }

    def set_params(self, **parameters) -> 'PoultryGBRegressor':
        """Set parameters (scikit-learn interface)."""
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self

================
File: models/model_factory.py
================
from typing import Dict, Optional, Any
import os
import sys

# Add the parent directory to sys.path if needed
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

# Import models
from models.polynomial_regression import PoultryWeightPredictor
from models.gradient_boosting import PoultryGBRegressor
from models.svr_model import PoultrySVR
from models.random_forest import PoultryRandomForest

class ModelFactory:
    """Enhanced Model Factory with Random Forest support."""

    # Define valid parameters for each model type
    VALID_PARAMS = {
        'polynomial': {'degree', 'fit_intercept', 'include_bias'},
        'gradient_boosting': {
            'n_estimators', 'learning_rate', 'max_depth', 'min_samples_split',
            'min_samples_leaf', 'subsample', 'random_state', 'early_stopping_rounds',
            'validation_fraction'
        },
        'svr': {
            'kernel', 'C', 'epsilon', 'gamma', 'cache_size', 'max_iter',
            'random_state'
        },
        'random_forest': {
            'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf',
            'max_features', 'random_state', 'n_jobs', 'bootstrap', 'oob_score'
        }
    }
    
    @staticmethod
    def get_model(model_type: str, params: Optional[Dict] = None) -> Any:
        """Create and return a model instance."""
        models = {
            'polynomial': PoultryWeightPredictor,
            'gradient_boosting': PoultryGBRegressor,
            'svr': PoultrySVR,
            'random_forest': PoultryRandomForest
        }
        
        if model_type.lower() not in models:
            raise ValueError(f"Unknown model type: {model_type}. Available models: {list(models.keys())}")
            
        model_class = models[model_type.lower()]
        
        # Filter parameters based on model type
        if params is not None:
            valid_params = ModelFactory.VALID_PARAMS[model_type.lower()]
            filtered_params = {k: v for k, v in params.items() if k in valid_params}
            return model_class(**filtered_params)
        
        return model_class()
            
    @staticmethod
    def get_available_models() -> Dict:
        """Get list of available models with descriptions."""
        return {
            'polynomial': {
                'name': 'Polynomial Regression',
                'description': 'Traditional polynomial regression for baseline predictions.',
                'strengths': [
                    'Simple and interpretable',
                    'Fast training and prediction',
                    'Good for basic non-linear relationships',
                    'Low computational requirements'
                ],
                'limitations': [
                    'May overfit with high polynomial degrees',
                    'Sensitive to outliers',
                    'Limited complexity handling',
                    'Requires careful feature scaling'
                ],
                'use_cases': [
                    'Initial baseline modeling',
                    'Simple trend analysis',
                    'When interpretability is crucial'
                ]
            },
            'gradient_boosting': {
                'name': 'Gradient Boosting',
                'description': 'Advanced ensemble learning model for complex patterns.',
                'strengths': [
                    'Handles non-linear relationships well',
                    'Robust to outliers',
                    'Provides feature importance',
                    'High prediction accuracy'
                ],
                'limitations': [
                    'More computationally intensive',
                    'Requires more hyperparameter tuning',
                    'Can overfit if not properly configured',
                    'Less interpretable than simpler models'
                ],
                'use_cases': [
                    'Complex pattern recognition',
                    'When high accuracy is needed',
                    'Large dataset handling'
                ]
            },
            'svr': {
                'name': 'Support Vector Regression',
                'description': 'Advanced kernel-based regression for robust predictions.',
                'strengths': [
                    'Excellent generalization',
                    'Robust to outliers',
                    'Handles non-linear relationships well',
                    'Works well with medium-sized datasets'
                ],
                'limitations': [
                    'Slower training on large datasets',
                    'Requires careful kernel selection',
                    'Memory intensive for large datasets',
                    'Less intuitive feature importance'
                ],
                'use_cases': [
                    'Robust regression needs',
                    'When polynomial regression overfits',
                    'Medium-sized datasets'
                ]
            },
            'random_forest': {
                'name': 'Random Forest',
                'description': 'Ensemble learning model using multiple decision trees.',
                'strengths': [
                    'Excellent handling of non-linear relationships',
                    'Built-in feature importance',
                    'Less prone to overfitting',
                    'Can handle outliers well',
                    'Provides uncertainty estimates'
                ],
                'limitations': [
                    'More complex than single models',
                    'Requires more memory for large forests',
                    'Slower prediction time than simple models',
                    'May struggle with extrapolation'
                ],
                'use_cases': [
                    'Complex feature interactions',
                    'When feature importance is needed',
                    'Robust prediction requirements',
                    'Medium to large datasets'
                ]
            }
        }
    
    @staticmethod
    def get_model_params(model_type: str) -> Dict:
        """Get default parameters for each model type."""
        params = {
            'polynomial': {
                'degree': {
                    'default': 2,
                    'range': (1, 5),
                    'type': 'int',
                    'description': 'Polynomial degree for feature transformation'
                },
                'fit_intercept': {
                    'default': True,
                    'type': 'bool',
                    'description': 'Whether to calculate the intercept for this model'
                },
                'include_bias': {
                    'default': True,
                    'type': 'bool',
                    'description': 'Whether to include a bias column in polynomial features'
                }
            },
            'gradient_boosting': {
                'n_estimators': {
                    'default': 100,
                    'range': (50, 500),
                    'type': 'int',
                    'description': 'Number of boosting stages'
                },
                'learning_rate': {
                    'default': 0.1,
                    'range': (0.01, 0.3),
                    'type': 'float',
                    'description': 'Learning rate shrinks the contribution of each tree'
                },
                'max_depth': {
                    'default': 3,
                    'range': (2, 10),
                    'type': 'int',
                    'description': 'Maximum depth of the individual trees'
                },
                'validation_fraction': {
                    'default': 0.1,
                    'range': (0.1, 0.3),
                    'type': 'float',
                    'description': 'Fraction of training data to use for early stopping'
                },
                'early_stopping_rounds': {
                    'default': 10,
                    'range': (5, 50),
                    'type': 'int',
                    'description': 'Number of rounds with no improvement before early stopping'
                }
            },
            'svr': {
                'kernel': {
                    'default': 'rbf',
                    'options': ['rbf', 'linear', 'poly'],
                    'type': 'select',
                    'description': 'Kernel type for non-linear relationships'
                },
                'C': {
                    'default': 1.0,
                    'range': (0.1, 10.0),
                    'type': 'float',
                    'description': 'Regularization parameter'
                },
                'epsilon': {
                    'default': 0.1,
                    'range': (0.01, 1.0),
                    'type': 'float',
                    'description': 'Epsilon in the epsilon-SVR model'
                },
                'gamma': {
                    'default': 'scale',
                    'options': ['scale', 'auto'],
                    'type': 'select',
                    'description': 'Kernel coefficient'
                }
            },
            'random_forest': {
                'n_estimators': {
                    'default': 100,
                    'range': (50, 500),
                    'type': 'int',
                    'description': 'Number of trees in the forest'
                },
                'max_depth': {
                    'default': None,
                    'range': (3, 20),
                    'type': 'int',
                    'description': 'Maximum depth of the trees (None for unlimited)'
                },
                'min_samples_split': {
                    'default': 2,
                    'range': (2, 10),
                    'type': 'int',
                    'description': 'Minimum samples required to split an internal node'
                },
                'min_samples_leaf': {
                    'default': 1,
                    'range': (1, 5),
                    'type': 'int',
                    'description': 'Minimum samples required to be at a leaf node'
                },
                'max_features': {
                    'default': 'sqrt',
                    'options': ['sqrt', 'log2'],
                    'type': 'select',
                    'description': 'Number of features to consider for best split'
                },
                'bootstrap': {
                    'default': True,
                    'type': 'bool',
                    'description': 'Whether to use bootstrap samples'
                },
                'oob_score': {
                    'default': False,
                    'type': 'bool',
                    'description': 'Whether to use out-of-bag samples to estimate generalization score'
                }
            }
        }
        
        if model_type.lower() not in params:
            raise ValueError(f"Unknown model type: {model_type}")
        
        return params[model_type.lower()]

    @staticmethod
    def get_param_descriptions(model_type: str) -> Dict[str, str]:
        """Get parameter descriptions for a specific model type."""
        params = ModelFactory.get_model_params(model_type)
        return {param: info['description'] for param, info in params.items()}

    @staticmethod
    def get_model_comparison_metrics() -> Dict:
        """Define metrics for model comparison."""
        return {
            'mse': {
                'name': 'Mean Squared Error',
                'lower_is_better': True,
                'format': '.4f'
            },
            'rmse': {
                'name': 'Root Mean Squared Error',
                'lower_is_better': True,
                'format': '.4f'
            },
            'r2': {
                'name': 'R² Score',
                'lower_is_better': False,
                'format': '.4f'
            },
            'mae': {
                'name': 'Mean Absolute Error',
                'lower_is_better': True,
                'format': '.4f'
            },
            'mape': {
                'name': 'Mean Absolute Percentage Error',
                'lower_is_better': True,
                'format': '.2f'
            },
            'oob_score': {
                'name': 'Out-of-Bag Score',
                'lower_is_better': False,
                'format': '.4f'
            }
        }

    @staticmethod
    def suggest_model(data_characteristics: Dict) -> str:
        """Suggest best model based on data characteristics."""
        n_samples = data_characteristics.get('n_samples', 0)
        n_features = data_characteristics.get('n_features', 0)
        has_outliers = data_characteristics.get('has_outliers', False)
        complexity = data_characteristics.get('complexity', 'medium')
        
        if n_samples < 100:
            return 'polynomial'  # Simple model for small datasets
        elif has_outliers and n_samples < 1000:
            if complexity == 'high':
                return 'random_forest'  # Complex patterns with outliers
            return 'svr'  # Robust to outliers
        elif complexity == 'high' and n_samples >= 1000:
            return 'gradient_boosting'  # Complex patterns, large datasets
        else:
            return 'random_forest'  # Good balance of performance and robustness

================
File: models/polynomial_regression.py
================
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error

from sklearn.base import BaseEstimator, RegressorMixin
import numpy as np
import pandas as pd
import joblib
import os
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime

class PoultryWeightPredictor(BaseEstimator, RegressorMixin):
    """Polynomial Regression model for poultry weight prediction."""
    
    def __init__(self, degree=2, fit_intercept=True, include_bias=True):
        """
        Initialize the Polynomial Regression model.
        
        Args:
            degree: Degree of polynomial features
            fit_intercept: Whether to fit intercept in linear regression
            include_bias: Whether to include bias term in polynomial features
        """
        self.degree = int(degree)
        self.fit_intercept = fit_intercept
        self.include_bias = include_bias
        
        # Initialize model pipeline
        self.model = Pipeline([
            ('poly', PolynomialFeatures(
                degree=self.degree,
                include_bias=self.include_bias
            )),
            ('regressor', LinearRegression(
                fit_intercept=self.fit_intercept
            ))
        ])
        
        # Initialize state variables
        self._is_trained = False
        self.feature_names_ = None
        self._feature_importances = None
        self.training_metadata = {}
    
    def _validate_input_data(self, X: np.ndarray, y: Optional[np.ndarray] = None, is_training: bool = True):
        """Validate input data."""
        if X is None:
            raise ValueError("Input features cannot be None")
        if len(X) == 0:
            raise ValueError("Input features cannot be empty")
        if is_training:
            if y is None:
                raise ValueError("Target values cannot be None for training")
            if len(X) != len(y):
                raise ValueError("Number of samples in features and target must match")

    def fit(self, X, y):
        """Fit method for scikit-learn compatibility."""
        return self.train(X, y)

    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
            feature_names: Optional[List[str]] = None) -> 'PoultryWeightPredictor':
        """
        Train the model.
        
        Args:
            X_train: Training features
            y_train: Training targets
            feature_names: Optional list of feature names
        """
        try:
            # Validate input data
            self._validate_input_data(X_train, y_train, is_training=True)
            
            # Convert inputs to numpy arrays
            X_train = np.asarray(X_train)
            y_train = np.asarray(y_train)
            
            # Store feature names if provided or create default ones
            if feature_names is not None:
                if len(feature_names) != X_train.shape[1]:
                    raise ValueError(f"Number of feature names ({len(feature_names)}) does not match number of features ({X_train.shape[1]})")
                self.feature_names_ = feature_names
            else:
                self.feature_names_ = [f'feature_{i}' for i in range(X_train.shape[1])]
            
            # Train model
            print(f"Training Polynomial Regression model with degree {self.degree}")
            self.model.fit(X_train, y_train)
            
            # Get polynomial features and coefficients
            poly = self.model.named_steps['poly']
            regressor = self.model.named_steps['regressor']
            
            # Store feature importance information
            self._feature_importances = np.abs(regressor.coef_)
            
            # Mark as trained
            self._is_trained = True
            print("Model training completed successfully, _is_trained set to True")

            # Store training metadata
            self.training_metadata = {
                'n_samples': len(X_train),
                'n_features': X_train.shape[1],
                'n_polynomial_features': len(self._feature_importances),
                'training_date': datetime.now().isoformat(),
                'parameters': self.get_params(),
                'feature_names': self.feature_names_
            }
            
            return self
            
        except Exception as e:
            print(f"Error during training: {str(e)}")
            import traceback
            print(traceback.format_exc())
            raise

            
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions."""
        if not self._is_trained:
            raise ValueError("Model must be trained before making predictions")
            
        self._validate_input_data(X, is_training=False)
        return self.model.predict(X)
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Tuple[Dict[str, float], np.ndarray]:
        """Evaluate model performance."""
        if not self._is_trained:
            raise ValueError("Model must be trained before evaluation")
        
        try:
            y_pred = self.predict(X_test)
            
            metrics = {
                'mse': mean_squared_error(y_test, y_pred),
                'rmse': root_mean_squared_error(y_test, y_pred),
                'r2': r2_score(y_test, y_pred),
                'mae': mean_absolute_error(y_test, y_pred),
                'mape': np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            }
            
            return metrics, y_pred
            
        except Exception as e:
            print(f"Error during evaluation: {str(e)}")
            raise
    
    def get_feature_importance(self, feature_names: Optional[List[str]] = None) -> Dict[str, float]:
        """Get feature importance scores."""
        if not self._is_trained:
            raise ValueError("Model must be trained before getting feature importance")
        
        try:
            poly = self.model.named_steps['poly']
            
            if feature_names is None:
                feature_names = self.feature_names_
            
            if feature_names is None:
                feature_names = [f'feature_{i}' for i in range(len(self._feature_importances))]
            
            # Get polynomial feature names
            poly_features = poly.get_feature_names_out(feature_names)
            
            # Create importance dictionary
            importance_dict = dict(zip(poly_features, np.abs(self._feature_importances)))
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
            
        except Exception as e:
            print(f"Error getting feature importance: {str(e)}")
            raise
    
    def save(self, filepath: str):
        """Save the trained model."""
        if not self._is_trained:
            raise ValueError("Model must be trained before saving")
        
        try:
            save_dict = {
                'model': self.model,
                'feature_names': self.feature_names_,
                'is_trained': self._is_trained,
                'feature_importances': self._feature_importances,
                'training_metadata': self.training_metadata,
                'parameters': self.get_params(),
                'save_timestamp': datetime.now().isoformat()
            }
            
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            joblib.dump(save_dict, filepath)
            print(f"Model saved successfully to {filepath}")
            
        except Exception as e:
            print(f"Error saving model: {str(e)}")
            raise
    
    @classmethod
    def load(cls, filepath: str) -> 'PoultryWeightPredictor':
        """Load a saved model."""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")
        
        try:
            save_dict = joblib.load(filepath)
            
            # Create new instance with saved parameters
            instance = cls(**save_dict['parameters'])
            
            # Restore saved state
            instance.model = save_dict['model']
            instance.feature_names_ = save_dict['feature_names']
            instance._feature_importances = save_dict['feature_importances']
            instance._is_trained = save_dict['is_trained']
            instance.training_metadata = save_dict['training_metadata']
            
            return instance
            
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            raise
    
    def get_params(self, deep=True) -> Dict:
        """Get parameters (scikit-learn interface)."""
        return {
            'degree': self.degree,
            'fit_intercept': self.fit_intercept,
            'include_bias': self.include_bias
        }
    
    def set_params(self, **parameters) -> 'PoultryWeightPredictor':
        """Set parameters (scikit-learn interface)."""
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        
        # Update model pipeline with new parameters
        self.model = Pipeline([
            ('poly', PolynomialFeatures(
                degree=self.degree,
                include_bias=self.include_bias
            )),
            ('regressor', LinearRegression(
                fit_intercept=self.fit_intercept
            ))
        ])
        
        return self

================
File: models/random_forest.py
================
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import os
import numpy as np
from config.settings import MODEL_SAVE_PATH

class PoultryRandomForest:
    def __init__(self, **kwargs):
        """
        Initialize the Random Forest model with parameters.
        """
        # Default parameters
        self.default_params = {
            'n_estimators': 100,
            'max_depth': None,
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'max_features': 'auto',
            'bootstrap': True,
            'n_jobs': -1,
            'random_state': 42,
            'oob_score': False
        }
        
        # Update defaults with provided parameters
        self.params = {**self.default_params, **kwargs}
        
        # Initialize the model with parameters
        self.model = RandomForestRegressor(**self.params)
        self._is_trained = False
        self.feature_names = None
        
    @property
    def is_trained(self):
        """Check if the model is trained."""
        return self._is_trained
        
    def set_feature_names(self, feature_names):
        """Set feature names for the model."""
        self.feature_names = feature_names
        
    def train(self, X_train, y_train, feature_names=None):
        """Train the model."""
        if X_train is None or y_train is None:
            raise ValueError("Training data cannot be None")
        if len(X_train) == 0 or len(y_train) == 0:
            raise ValueError("Training data cannot be empty")
            
        try:
            print("Training Random Forest model...")
            print(f"Training data shapes: X={X_train.shape}, y={y_train.shape}")
            print("Model parameters:", self.params)
            
            if feature_names is not None:
                self.feature_names = feature_names
            
            self.model.fit(X_train, y_train)
            self._is_trained = True
            print("Model trained successfully")
            
            if hasattr(self.model, 'feature_importances_'):
                print("Feature importances available")
            
            if self.params['oob_score']:
                print(f"OOB Score: {self.model.oob_score_:.4f}")
                
            return self
            
        except Exception as e:
            print(f"Error during training: {str(e)}")
            raise
            
    def predict(self, X):
        """Make predictions using the trained model."""
        if not self._is_trained:
            raise ValueError("Model needs to be trained before making predictions")
        
        if X is None or len(X) == 0:
            raise ValueError("Input data cannot be empty")
            
        try:
            return self.model.predict(X)
        except Exception as e:
            print(f"Error during prediction: {str(e)}")
            raise
    
    def evaluate(self, X_test, y_test):
        """Evaluate the model performance."""
        if not self._is_trained:
            raise ValueError("Model needs to be trained before evaluation")
            
        try:
            # Make predictions
            y_pred = self.predict(X_test)
            
            # Calculate metrics
            metrics = {
                'mse': mean_squared_error(y_test, y_pred),
                'rmse': mean_squared_error(y_test, y_pred, squared=False),
                'r2': r2_score(y_test, y_pred),
                'mae': np.mean(np.abs(y_test - y_pred)),
                'mape': np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            }
            
            # Add OOB score if available
            if hasattr(self.model, 'oob_score_'):
                metrics['oob_score'] = self.model.oob_score_
            
            return metrics, y_pred
            
        except Exception as e:
            print(f"Error during evaluation: {str(e)}")
            raise
    
    def get_feature_importance(self):
        """Get feature importance based on the trained model."""
        if not self._is_trained:
            raise ValueError("Model needs to be trained before getting feature importance")
            
        try:
            if not hasattr(self.model, 'feature_importances_'):
                raise ValueError("Model does not support feature importance")
                
            # Get feature importances
            importances = self.model.feature_importances_
            
            # Use stored feature names if available, otherwise use indices
            feature_names = self.feature_names if self.feature_names is not None else [f'Feature_{i}' for i in range(len(importances))]
            
            # Create feature importance dictionary
            importance_dict = dict(zip(feature_names, importances))
            
            # Sort by importance
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
            
        except Exception as e:
            print(f"Error getting feature importance: {str(e)}")
            raise
    
    def save(self, filepath):
        """Save the model to a file."""
        if not self._is_trained:
            raise ValueError("Model needs to be trained before saving")
            
        try:
            # Ensure directory exists
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            # Save the entire object
            joblib.dump(self, filepath)
            print(f"Model saved to {filepath}")
        except Exception as e:
            print(f"Error saving model: {str(e)}")
            raise
    
    @classmethod
    def load(cls, filepath):
        """Load a model from a file."""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")
            
        try:
            # Load the model
            model = joblib.load(filepath)
            if not isinstance(model, cls):
                raise ValueError("Loaded file is not a valid model")
            return model
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            raise
    
    def get_params(self):
        """Get the current model parameters."""
        return self.params
    
    def set_params(self, **params):
        """Update model parameters."""
        self.params.update(params)
        self.model.set_params(**self.params)
        self._is_trained = False  # Reset trained status since parameters changed

================
File: models/svr_model.py
================
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.base import BaseEstimator, RegressorMixin
import numpy as np
import pandas as pd
import joblib
import os
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime

class PoultrySVR(BaseEstimator, RegressorMixin):
    """Support Vector Regression model for poultry weight prediction."""
    
    def __init__(self, kernel='rbf', C=1.0, epsilon=0.1, gamma='scale',
                 cache_size=200, max_iter=-1, random_state=42):
        """
        Initialize the SVR model with parameters.
        
        Args:
            kernel: Kernel type ('rbf', 'linear', 'poly')
            C: Regularization parameter
            epsilon: Epsilon in epsilon-SVR model
            gamma: Kernel coefficient
            cache_size: Kernel cache size in MB
            max_iter: Maximum iterations
            random_state: Random state for reproducibility
        """
        self.kernel = kernel
        self.C = float(C)
        self.epsilon = float(epsilon)
        self.gamma = gamma
        self.cache_size = int(cache_size)
        self.max_iter = int(max_iter)
        self.random_state = int(random_state)
        
        # Initialize model and scaler
        self.model = None
        self.scaler = StandardScaler()
        self._is_trained = False
        self.feature_names_ = None
        self.training_metadata = {}
    
    def _get_model_params(self) -> Dict:
        """Get parameters for SVR model."""
        return {
            'kernel': self.kernel,
            'C': self.C,
            'epsilon': self.epsilon,
            'gamma': self.gamma,
            'cache_size': self.cache_size,
            'max_iter': self.max_iter
        }
        
    def fit(self, X, y):
        """Fit method for scikit-learn compatibility."""
        return self.train(X, y)
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
              feature_names: Optional[List[str]] = None) -> 'PoultrySVR':
        """
        Train the SVR model with scaled features.
        """
        try:
            # Store feature names if provided
            if feature_names is not None:
                self.feature_names_ = feature_names
            
            # Scale features
            X_scaled = self.scaler.fit_transform(X_train)
            
            # Initialize and train model
            self.model = SVR(**self._get_model_params())
            self.model.fit(X_scaled, y_train)
            
            # Mark as trained
            self._is_trained = True
            
            # Store training metadata
            self.training_metadata = {
                'n_samples': len(X_train),
                'n_features': X_train.shape[1],
                'n_support_vectors': len(self.model.support_vectors_),
                'training_date': datetime.now().isoformat(),
                'parameters': self._get_model_params()
            }
            
            return self
            
        except Exception as e:
            print(f"Error during training: {str(e)}")
            raise
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions using the trained model."""
        if not self._is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Tuple[Dict[str, float], np.ndarray]:
        """Evaluate model performance."""
        if not self._is_trained:
            raise ValueError("Model must be trained before evaluation")
        
        try:
            y_pred = self.predict(X_test)
            
            metrics = {
                'mse': mean_squared_error(y_test, y_pred),
                'rmse': mean_squared_error(y_test, y_pred, squared=False),
                'r2': r2_score(y_test, y_pred),
                'mae': mean_absolute_error(y_test, y_pred),
                'mape': np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            }
            
            return metrics, y_pred
            
        except Exception as e:
            print(f"Error during evaluation: {str(e)}")
            raise
    
    def get_feature_importance(self, feature_names: Optional[List[str]] = None) -> Dict[str, float]:
        """
        Get feature importance scores.
        Note: SVR doesn't provide direct feature importance, so we use coefficient magnitudes for linear kernel
        and provide uniform importance for other kernels.
        """
        if not self._is_trained:
            raise ValueError("Model must be trained before getting feature importance")
            
        try:
            if feature_names is None:
                feature_names = self.feature_names_ or [f'feature_{i}' for i in range(len(self.scaler.mean_))]
            
            # For linear kernel, use coefficient magnitudes
            if self.kernel == 'linear':
                importances = np.abs(self.model.coef_[0])
            else:
                # For non-linear kernels, provide uniform importance
                importances = np.ones(len(feature_names)) / len(feature_names)
            
            # Create and sort importance dictionary
            importance_dict = dict(zip(feature_names, importances))
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
            
        except Exception as e:
            print(f"Error getting feature importance: {str(e)}")
            raise
    
    def save(self, filepath: str):
        """Save the trained model."""
        if not self._is_trained:
            raise ValueError("Model must be trained before saving")
        
        try:
            save_dict = {
                'model': self.model,
                'scaler': self.scaler,
                'feature_names': self.feature_names_,
                'is_trained': self._is_trained,
                'training_metadata': self.training_metadata,
                'parameters': self._get_model_params(),
                'save_timestamp': datetime.now().isoformat()
            }
            
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            joblib.dump(save_dict, filepath)
            print(f"Model saved successfully to {filepath}")
            
        except Exception as e:
            print(f"Error saving model: {str(e)}")
            raise
    
    @classmethod
    def load(cls, filepath: str) -> 'PoultrySVR':
        """Load a saved model."""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")
        
        try:
            save_dict = joblib.load(filepath)
            
            # Create new instance with saved parameters
            instance = cls(**save_dict['parameters'])
            
            # Restore saved state
            instance.model = save_dict['model']
            instance.scaler = save_dict['scaler']
            instance.feature_names_ = save_dict['feature_names']
            instance._is_trained = save_dict['is_trained']
            instance.training_metadata = save_dict['training_metadata']
            
            return instance
            
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            raise
    
    def get_params(self, deep=True) -> Dict:
        """Get parameters (scikit-learn interface)."""
        return {
            'kernel': self.kernel,
            'C': self.C,
            'epsilon': self.epsilon,
            'gamma': self.gamma,
            'cache_size': self.cache_size,
            'max_iter': self.max_iter,
            'random_state': self.random_state
        }
    
    def set_params(self, **parameters) -> 'PoultrySVR':
        """Set parameters (scikit-learn interface)."""
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self

================
File: pages/1_Data_Upload.py
================
import streamlit as st
import pandas as pd
from utils.data_processor import DataProcessor
from utils.visualizations import Visualizer

def app():
    st.title("📤 Data Upload and Preview")
    
    # Initialize objects
    data_processor = DataProcessor()
    
    # File upload
    uploaded_file = st.file_uploader(
        "Upload your CSV file", 
        type=['csv'],
        help="Upload a CSV file containing poultry data"
    )
    
    if uploaded_file is not None:
        try:
            # Debug information
            st.write("File uploaded successfully")
            st.write(f"Filename: {uploaded_file.name}")
            
            # Read the data
            df = pd.read_csv(uploaded_file)
            
            # Debug information
            st.write(f"Data shape: {df.shape}")
            st.write("Columns found:", df.columns.tolist())
            
            # Display raw data preview before processing
            st.subheader("Raw Data Preview")
            st.dataframe(df.head())
            
            # Validate columns
            is_valid, missing_cols = data_processor.validate_columns(df)
            
            if not is_valid:
                st.error(f"Missing required columns: {', '.join(missing_cols)}")
                st.stop()
                
            # Display column information
            st.subheader("Column Information")
            st.write(df.dtypes)
            
            # Show number of null values
            st.subheader("Null Values Count")
            st.write(df.isnull().sum())
            
            # Process the data
            df_processed = data_processor.preprocess_data(df)
            
            # Debug information
            st.write(f"Processed data shape: {df_processed.shape}")
            
            # Display processed data preview
            st.subheader("Processed Data Preview")
            st.dataframe(df_processed.head())
            
            # Save to session state
            st.session_state['data'] = df_processed
            
            # Display basic statistics
            st.subheader("Basic Statistics")
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Number of Records", len(df_processed))
                st.metric("Number of Features", len(df_processed.columns)-1)
            
            with col2:
                st.metric("Missing Values", df_processed.isnull().sum().sum())
                st.metric("Duplicate Records", df_processed.duplicated().sum())
            
            # Display summary statistics
            st.subheader("Summary Statistics")
            st.write(df_processed.describe())
            
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
            st.write("Error details:", e)
            import traceback
            st.write("Traceback:", traceback.format_exc())
    
    else:
        st.info("Please upload a CSV file to begin.")
        
        # Show sample data format
        st.subheader("Required Data Format")
        sample_data = pd.DataFrame({
            'Int Temp': [30.5, 31.2, 29.8],
            'Int Humidity': [65, 68, 70],
            'Air Temp': [28.5, 29.0, 27.5],
            'Wind Speed': [4.2, 3.8, 4.5],
            'Feed Intake': [150, 155, 148],
            'Weight': [1200, 1250, 1180]
        })
        
        st.write("Your CSV file should have these columns:")
        st.dataframe(sample_data)
        
        # Download sample template
        st.download_button(
            label="Download Sample Template",
            data=sample_data.to_csv(index=False),
            file_name="sample_template.csv",
            mime="text/csv"
        )

if __name__ == "__main__":
    app()

================
File: pages/2_Data_Analysis.py
================
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px 
from utils.data_processor import DataProcessor
from utils.visualizations import Visualizer
from config.settings import REQUIRED_COLUMNS

def app():
    st.title("📊 Data Analysis")
    
    # Check if data exists in session state
    if 'data' not in st.session_state:
        st.error("Please upload data in the Data Upload page first!")
        st.stop()
        
    # Initialize objects
    data_processor = DataProcessor()
    visualizer = Visualizer()
    df = st.session_state['data']
    
    # Sidebar for analysis options
    st.sidebar.subheader("Analysis Options")
    analysis_type = st.sidebar.selectbox(
        "Select Analysis Type",
        ["Time Series Analysis", "Feature Relationships", "Outlier Detection"]
    )
    
    if analysis_type == "Time Series Analysis":
        st.subheader("Weight Progression Over Time")
        weight_plot = visualizer.plot_weight_over_time(df)
        st.plotly_chart(weight_plot, use_container_width=True)
        
        # Show growth rate
        st.subheader("Growth Rate Analysis")
        df['Growth Rate'] = df['Weight'].pct_change() * 100
        growth_plot = visualizer.plot_feature_distribution(df, 'Growth Rate')
        st.plotly_chart(growth_plot, use_container_width=True)
        
    elif analysis_type == "Feature Relationships":
        st.subheader("Feature Relationships")
        
        # Select features to compare
        col1, col2 = st.columns(2)
        with col1:
            feature_1 = st.selectbox("Select first feature", REQUIRED_COLUMNS)
        with col2:
            feature_2 = st.selectbox("Select second feature", 
                                   [col for col in REQUIRED_COLUMNS if col != feature_1])
        
        # Create scatter plot
        fig = px.scatter(df, x=feature_1, y=feature_2, 
                        
                        title=f"{feature_1} vs {feature_2}")
        st.plotly_chart(fig, use_container_width=True)
        
        # Show correlation coefficient
        correlation = df[feature_1].corr(df[feature_2])
        st.metric("Correlation Coefficient", f"{correlation:.3f}")
        
    else:  # Outlier Detection
            st.subheader("Outlier Detection")
            
            # Show comprehensive outlier analysis
            st.markdown("### Overall Outlier Status")
            outliers_by_column = {
                col: data_processor.detect_outliers(df, col).any()
                for col in REQUIRED_COLUMNS
            }
            
            # Display overall outlier status
            has_any_outliers = any(outliers_by_column.values())
            st.metric(
                "Features with Outliers", 
                sum(outliers_by_column.values()),
                help="Number of features that contain outliers"
            )
            
            # Show which features have outliers
            if has_any_outliers:
                st.markdown("#### Features Containing Outliers:")
                for col, has_outliers in outliers_by_column.items():
                    if has_outliers:
                        st.markdown(f"- {col}")
            
            # Separator
            st.markdown("---")
            
            # Detailed analysis for selected feature
            st.markdown("### Detailed Feature Analysis")
            # Select feature for outlier detection
            feature = st.selectbox(
                "Select feature for detailed outlier analysis", 
                REQUIRED_COLUMNS
            )
            
            # Detect outliers for selected feature
            outliers = data_processor.detect_outliers(df, feature)
            
            # Show statistics for selected feature
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Number of Outliers", outliers.sum())
            with col2:
                st.metric("Percentage of Outliers", f"{(outliers.sum()/len(df))*100:.2f}%")
            
            # Plot with outliers highlighted
            fig = px.scatter(
                df,
                x=df.index,
                y=feature,
                color=outliers,
                title=f"Outliers in {feature}",
                color_discrete_map={True: "red", False: "blue"},
                labels={"index": "Sample Index", feature: feature}
            )
            fig.update_layout(
                showlegend=True,
                legend_title="Is Outlier"
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Show summary statistics for outliers vs non-outliers
            if outliers.sum() > 0:
                st.markdown("#### Summary Statistics")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Normal Values**")
                    st.write(df[~outliers][feature].describe().round(2))
                with col2:
                    st.markdown("**Outliers**")
                    st.write(df[outliers][feature].describe().round(2))


    # Download analyzed data
    st.sidebar.markdown("---")
    if st.sidebar.button("Download Analyzed Data"):
        csv = df.to_csv(index=False)
        st.sidebar.download_button(
            label="Download CSV",
            data=csv,
            file_name="analyzed_data.csv",
            mime="text/csv"
        )

if __name__ == "__main__":
    app()

================
File: pages/3_Model_Training.py
================
import streamlit as st
import pandas as pd
import numpy as np
import os
import joblib
import traceback
import hashlib
from datetime import datetime
from sklearn.model_selection import cross_val_score, cross_validate
from typing import Dict, Tuple, List, Any
from utils.data_processor import DataProcessor, FEATURE_COLUMNS
from utils.visualizations import Visualizer
from models.model_factory import ModelFactory
from config.settings import MODEL_SAVE_PATH
from utils.model_manager import ModelManager
from utils.model_management_ui import render_model_management_ui

VERSION = "2.0.0"

def generate_data_hash(df: pd.DataFrame) -> str:
    """Generate a hash of the training data for versioning."""
    return hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()

def analyze_data_characteristics(df: pd.DataFrame, data_processor: DataProcessor) -> Dict:
    """Analyze data characteristics for model selection and insights."""
    
    # Check for outliers in each feature column
    outliers_by_column = {
        col: data_processor.detect_outliers(df, col).any()
        for col in FEATURE_COLUMNS
    }
    
    characteristics = {
        'n_samples': len(df),
        'n_features': len(FEATURE_COLUMNS),
        'has_outliers': any(outliers_by_column.values()),
        'outliers_by_column': outliers_by_column,
        'missing_values': df.isnull().sum().sum(),
        'feature_correlations': df[FEATURE_COLUMNS].corr(),
        'data_size': 'large' if len(df) > 1000 else 'medium' if len(df) > 100 else 'small',
        'complexity': 'high' if len(df) > 1000 else 'medium',
        'feature_stats': {col: {
            'mean': df[col].mean(),
            'std': df[col].std(),
            'skew': df[col].skew(),
            'kurtosis': df[col].kurtosis(),
            'has_outliers': outliers_by_column[col]
        } for col in FEATURE_COLUMNS}
    }
    
    return characteristics

def validate_model_params(params: Dict, model_type: str, model_factory: ModelFactory) -> Tuple[bool, str]:
    """Enhanced parameter validation for model parameters."""
    param_info = model_factory.get_model_params(model_type)
    
    for param_name, value in params.items():
        if param_name not in param_info:
            return False, f"Unexpected parameter: {param_name}"
            
        if isinstance(value, (int, float)):
            if value <= 0:
                return False, f"Parameter {param_name} must be positive"
    return True, ""

def perform_cross_validation(model: Any, X: np.ndarray, y: np.ndarray, cv: int = 5) -> Dict[str, float]:
    """Perform cross-validation and return detailed scores."""
    try:
        scoring = {
            'r2': 'r2',
            'mse': 'neg_mean_squared_error',
            'mae': 'neg_mean_absolute_error'
        }
        
        scores = cross_validate(model, X, y, cv=cv, scoring=scoring)
        
        return {
            'cv_r2_mean': scores['test_r2'].mean(),
            'cv_r2_std': scores['test_r2'].std(),
            'cv_mse_mean': -scores['test_mse'].mean(),
            'cv_mse_std': scores['test_mse'].std(),
            'cv_mae_mean': -scores['test_mae'].mean(),
            'cv_mae_std': scores['test_mae'].std()
        }
    except Exception as e:
        st.warning(f"Cross-validation failed: {str(e)}")
        return {}

def get_model_recommendation(data_characteristics: Dict) -> Tuple[str, str, List[str]]:
    """Get model recommendations based on data characteristics."""
    reasons = []
    
    if data_characteristics['data_size'] == 'small':
        if not data_characteristics['has_outliers']:
            return 'polynomial', "Polynomial Regression", [
                "Small dataset size",
                "No significant outliers",
                "Good for interpretability"
            ]
    
    if data_characteristics['has_outliers']:
        reasons.append("Dataset contains outliers")
        
    if data_characteristics['data_size'] in ['medium', 'large']:
        reasons.append("Sufficient data for complex patterns")
        
    return 'gradient_boosting', "Gradient Boosting", reasons


def app():
    st.title("🎯 Model Training")
    
    # Initialize model manager
    model_manager = ModelManager()
    
    # Check if data exists in session state
    if 'data' not in st.session_state:
        st.error("Please upload data in the Data Upload page first!")
        st.stop()
    
    # Initialize objects
    data_processor = DataProcessor()
    visualizer = Visualizer()
    model_factory = ModelFactory()
    
    # Get and process data
    df = st.session_state['data']
    
    try:
        # Process data and analyze characteristics
        df_processed = data_processor.preprocess_data(df)
        data_characteristics = analyze_data_characteristics(df_processed, data_processor)
        data_hash = generate_data_hash(df_processed)
        
        # Save data_processor in session state
        st.session_state['data_processor'] = data_processor
        
        # Success message
        st.success(f"Data preprocessed successfully: {df_processed.shape[0]} rows")
        
        # Data Overview Section
        st.subheader("📊 Data Overview")
        
        # Basic metrics
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Samples", data_characteristics['n_samples'])
            st.metric("Features", data_characteristics['n_features'])
        with col2:
            st.metric("Missing Values", data_characteristics['missing_values'])
            st.metric("Contains Outliers", "Yes" if data_characteristics['has_outliers'] else "No")
        with col3:
            st.metric("Data Size", data_characteristics['data_size'].title())
            st.metric("Complexity", data_characteristics['complexity'].title())
        
        # Outlier Information
        if data_characteristics['has_outliers']:
            st.markdown("#### Outlier Information")
            st.write("**Features with Outliers:**")
            outlier_cols = [col for col, has_outliers in 
                        data_characteristics['outliers_by_column'].items() 
                        if has_outliers]
            
            # Create columns for outlier information
            n_cols = 3
            cols = st.columns(n_cols)
            for idx, col in enumerate(outlier_cols):
                with cols[idx % n_cols]:
                    st.write(f"• {col}")
                    stats = data_characteristics['feature_stats'][col]
                    st.write(f"  Mean: {stats['mean']:.2f}")
                    st.write(f"  Std: {stats['std']:.2f}")
        
        # Feature Statistics
        with st.expander("Feature Statistics"):
            stats_df = pd.DataFrame({
                col: {
                    'Mean': stats['mean'],
                    'Std Dev': stats['std'],
                    'Skewness': stats['skew'],
                    'Kurtosis': stats['kurtosis']
                }
                for col, stats in data_characteristics['feature_stats'].items()
            }).T
            
            st.dataframe(stats_df.style.format("{:.2f}"))
        
        # Feature Correlations
        with st.expander("Feature Correlations"):
            st.plotly_chart(
                visualizer.plot_correlation_matrix(
                    data_characteristics['feature_correlations']
                ),
                use_container_width=True
            )

    except Exception as e:
        st.error(f"Error preprocessing data: {str(e)}")
        st.code(traceback.format_exc())
        st.stop()

            #####################################
    # Get model recommendation
    recommended_model, rec_name, rec_reasons = get_model_recommendation(data_characteristics)
    
    # Sidebar - Model Selection with Recommendation
    st.sidebar.subheader("Model Selection")
    available_models = model_factory.get_available_models()
    
    selected_model_type = st.sidebar.selectbox(
        "Select Model Type",
        list(available_models.keys()),
        index=list(available_models.keys()).index(recommended_model),
        format_func=lambda x: f"{available_models[x]['name']} {'✨ (Recommended)' if x == recommended_model else ''}"
    )
    
    # Show recommendation reasoning
    if recommended_model == selected_model_type:
        st.sidebar.success(f"✨ Recommended model based on:")
        for reason in rec_reasons:
            st.sidebar.write(f"- {reason}")
    else:
        st.sidebar.info(f"💡 {rec_name} is recommended for your data based on its characteristics.")
    
    # Model Information
    with st.sidebar.expander("Model Details", expanded=True):
        st.write(f"**{available_models[selected_model_type]['name']}**")
        st.write(available_models[selected_model_type]['description'])
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**Strengths:**")
            for strength in available_models[selected_model_type]['strengths']:
                st.write(f"✓ {strength}")
        with col2:
            st.write("**Limitations:**")
            for limitation in available_models[selected_model_type]['limitations']:
                st.write(f"• {limitation}")
    
    # Training Configuration
    st.sidebar.subheader("Training Configuration")
    
    # Advanced settings toggle
    show_advanced = st.sidebar.checkbox("Show Advanced Settings", value=False)
    
    # Model parameters
    default_params = model_factory.get_model_params(selected_model_type)
    param_descriptions = model_factory.get_param_descriptions(selected_model_type)
    
    # Model parameters
    model_params = {}
    with st.sidebar.expander("Model Parameters", expanded=True):
        for param, param_info in default_params.items():
            # Skip random_state for all models except gradient_boosting
            if param == 'random_state' and selected_model_type != 'gradient_boosting':
                continue
                
            if param in ['subsample', 'min_samples_split'] and not show_advanced:
                continue

            if 'type' in param_info:
                if param_info['type'] == 'int':
                    model_params[param] = st.number_input(
                        param,
                        min_value=1,
                        value=param_info['default'],
                        help=param_descriptions[param]
                    )
                elif param_info['type'] == 'float':
                    # Different ranges for different parameters
                    min_val = param_info.get('range', (0.0, 1.0))[0]
                    max_val = param_info.get('range', (0.0, 1.0))[1]
                    step = 0.01 if param == 'learning_rate' else 0.1
                    
                    model_params[param] = st.slider(
                        param,
                        min_value=min_val,
                        max_value=max_val,
                        value=param_info['default'],
                        step=step,
                        format="%.3f",
                        help=param_descriptions[param]
                    )
                elif param_info['type'] == 'select':
                    model_params[param] = st.selectbox(
                        param,
                        options=param_info['options'],
                        index=param_info['options'].index(param_info['default']),
                        help=param_descriptions[param]
                    )

    # Add fixed parameters
    if selected_model_type == 'gradient_boosting':
        model_params['random_state'] = 42
    
    # Training settings
    st.sidebar.subheader("Training Settings")
    
    # Test size slider
    min_test_size = max(0.1, 1 / len(df_processed))
    test_size = st.sidebar.slider(
        "Test Set Size", 
        min_value=min_test_size,
        max_value=0.4,
        value=0.2,
        step=0.05,
        help="Proportion of dataset to include in the test split"
    )
    
    # Advanced training settings
    if show_advanced:
        with st.sidebar.expander("Advanced Training Settings"):
            use_cv = st.checkbox("Use Cross-validation", value=True)
            if use_cv:
                cv_folds = st.slider("Number of CV Folds", min_value=2, max_value=10, value=5)
            else:
                cv_folds = 0
            
            if selected_model_type == 'gradient_boosting':
                use_early_stopping = st.checkbox("Use Early Stopping", value=True)
                if use_early_stopping:
                    early_stopping_rounds = st.slider(
                        "Early Stopping Rounds",
                        min_value=5,
                        max_value=50,
                        value=10
                    )
                    model_params['early_stopping_rounds'] = early_stopping_rounds
    else:
        use_cv = False
        cv_folds = 0
    
    # Add fixed parameters
    model_params['random_state'] = 42

    ################################################################################

    # Main content
    st.subheader("Data Split and Model Training")
    
    # Show feature information
    with st.expander("Feature Information", expanded=True):
        st.write("Features being used:", FEATURE_COLUMNS)
        
        # Feature correlations heatmap
        if st.checkbox("Show Feature Correlations"):
            corr_matrix = data_characteristics['feature_correlations']
            st.plotly_chart(visualizer.plot_correlation_matrix(corr_matrix), use_container_width=True)
    
    # Prepare features
    try:
        X_train, X_test, y_train, y_test = data_processor.prepare_features(
            df_processed, 
            test_size=test_size
        )
        st.success("Features prepared successfully")
        
        # Data split information
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"Training set shape: {X_train.shape}")
        with col2:
            st.write(f"Test set shape: {X_test.shape}")
        
    except Exception as e:
        st.error(f"Error preparing features: {str(e)}")
        st.code(traceback.format_exc())
        st.stop()
    
    # Save test data
    st.session_state['test_data'] = {
        'X_test': X_test,
        'y_test': y_test
    }
   ################################################################################# 
    # Training section
    training_container = st.container()
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Create model instance
    model = model_factory.get_model(selected_model_type, model_params)
    
    # Train model button
    if st.button("Train Model", key='train_button'):
        try:
            # Training phase
            status_text.text("Training model...")
            progress_bar.progress(20)
            
        
            feature_names = list(X_train.columns) if hasattr(X_train, 'columns') else FEATURE_COLUMNS

             # Train model
            if hasattr(model, 'set_feature_names'):
                model.set_feature_names(feature_names)
                model.train(X_train, y_train)
            else:
                model.train(X_train, y_train, feature_names=feature_names)
            progress_bar.progress(40)

            
            # Cross-validation if enabled
            cv_results = {}
            if use_cv:
                status_text.text("Performing cross-validation...")
                cv_results = perform_cross_validation(model, X_train, y_train, cv_folds)
                progress_bar.progress(60)
            
            # Model evaluation
            status_text.text("Evaluating model...")
            metrics, y_pred = model.evaluate(X_test, y_test)
            progress_bar.progress(80)
            
            # Feature importance
            importance_dict = model.get_feature_importance()
            progress_bar.progress(90)
            
            # Create model metadata
            metadata = {
                'version': VERSION,
                'training_date': datetime.now().isoformat(),
                'data_hash': data_hash,
                'model_type': selected_model_type,
                'model_params': model_params,
                'feature_columns': FEATURE_COLUMNS,
                'metrics': metrics,
                'cv_results': cv_results,
                'feature_importance': importance_dict,
                'data_characteristics': {
                    'total_samples': len(df_processed),
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'features': len(FEATURE_COLUMNS),
                    'has_outliers': data_characteristics['has_outliers'],
                    'data_size': data_characteristics['data_size'],
                    'complexity': data_characteristics['complexity']
                },
                'training_config': {
                    'test_size': test_size,
                    'cross_validation': use_cv,
                    'cv_folds': cv_folds if use_cv else None,
                    'parameters': model_params
                }
            }
            
            # Store in ModelManager
            model_id = model_manager.save_model(
                        model_name=f"{available_models[selected_model_type]['name']}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}",
                        model=model,
                        metadata={
                            'version': VERSION,
                            'model_type': selected_model_type,
                            'metrics': metrics,
                            'predictions': y_pred.tolist(),  # Include predictions
                            'actual': y_test.tolist(),       # Include actual values
                            'data_characteristics': data_characteristics,
                            'training_config': {
                                'test_size': test_size,
                                'cross_validation': use_cv,
                                'cv_folds': cv_folds if use_cv else None,
                                'parameters': model_params
                            },
                            'feature_importance': importance_dict,
                            'data_hash': data_hash
                        }
                    )
            
            # Store in session state
            st.session_state['model'] = model
            st.session_state['model_metadata'] = metadata
            st.session_state['training_results'] = {
                'metrics': metrics,
                'predictions': y_pred,
                'feature_importance': importance_dict,
                'cv_results': cv_results,
                'test_size': test_size
            }
            
            progress_bar.progress(100)
            status_text.text("Training completed successfully!")
            st.success(f"Model trained and saved successfully with ID: {model_id}")
            
        except Exception as e:
            st.error(f"Error during training: {str(e)}")
            st.code(traceback.format_exc())
            progress_bar.empty()
            status_text.empty()

            ############################################################################

            # Display results if available
    if st.session_state.get('training_results'):
        results = st.session_state['training_results']
        metrics = results['metrics']
        y_pred = results['predictions']
        importance_dict = results['feature_importance']
        cv_results = results.get('cv_results', {})
        
        # Create tabs for different sections
        tabs = st.tabs(["Model Performance", "Cross-Validation", "Predictions", "Feature Importance", "Model Management"])
        
        # Model Performance tab
        with tabs[0]:
            st.subheader("Model Performance Metrics")
            
            # Basic metrics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Mean Squared Error", f"{metrics['mse']:.2f}")
            with col2:
                st.metric("Root MSE", f"{metrics['rmse']:.2f}")
            with col3:
                st.metric("R² Score", f"{metrics['r2']:.4f}")
            
            # Additional metrics if available
            if 'mae' in metrics:
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Mean Absolute Error", f"{metrics['mae']:.2f}")
                with col2:
                    st.metric("Mean Absolute % Error", f"{metrics.get('mape', 0):.2f}%")
            
            # Performance visualization
            st.plotly_chart(visualizer.plot_residuals(y_test, y_pred), use_container_width=True)
        
        # Cross-Validation tab
        with tabs[1]:
            st.subheader("Cross-Validation Results")
            if cv_results:
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("CV R² Score", f"{cv_results['cv_r2_mean']:.4f} ± {cv_results['cv_r2_std']:.4f}")
                    st.metric("CV MSE", f"{cv_results['cv_mse_mean']:.2f} ± {cv_results['cv_mse_std']:.2f}")
                with col2:
                    st.metric("CV MAE", f"{cv_results['cv_mae_mean']:.2f} ± {cv_results['cv_mae_std']:.2f}")
            else:
                st.info("Cross-validation was not performed. Enable it in Advanced Settings to see CV results.")

        # Predictions tab
        with tabs[2]:
            st.subheader("Actual vs Predicted Values")
            
            # Main prediction plot
            prediction_plot = visualizer.plot_actual_vs_predicted(
                st.session_state['test_data']['y_test'],
                y_pred
            )
            st.plotly_chart(prediction_plot, use_container_width=True)
            
            # Detailed predictions section
            with st.expander("Detailed Predictions Analysis", expanded=False):
                # Summary statistics
                errors = np.abs(y_test - y_pred)
                rel_errors = (errors / y_test) * 100
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Average Error", f"{np.mean(errors):.2f}")
                    st.metric("Max Error", f"{np.max(errors):.2f}")
                with col2:
                    st.metric("Average % Error", f"{np.mean(rel_errors):.2f}%")
                    st.metric("Max % Error", f"{np.max(rel_errors):.2f}%")
                
                # Detailed predictions table
                if st.checkbox("Show detailed predictions"):
                    n_examples = st.slider("Number of examples", min_value=5, max_value=len(y_pred), value=10)
                    examples = pd.DataFrame({
                        'Actual Weight': st.session_state['test_data']['y_test'][:n_examples],
                        'Predicted Weight': y_pred[:n_examples],
                        'Absolute Error': errors[:n_examples],
                        'Relative Error (%)': rel_errors[:n_examples]
                    })
                    st.dataframe(examples.style.highlight_max(axis=0))
                    
                    # Download predictions
                    st.download_button(
                        label="Download All Predictions",
                        data=examples.to_csv(index=False),
                        file_name="model_predictions.csv",
                        mime="text/csv"
                    )
        
        # Feature Importance tab
        with tabs[3]:
            st.subheader("Feature Importance Analysis")
            
            # Feature importance plot
            importance_plot = visualizer.plot_feature_importance(
                list(importance_dict.keys()),
                list(importance_dict.values())
            )
            st.plotly_chart(importance_plot, use_container_width=True)
            
            # Detailed feature analysis
            with st.expander("Feature Importance Details"):
                # Create and sort importance DataFrame
                importance_df = pd.DataFrame({
                    'Feature': importance_dict.keys(),
                    'Importance': importance_dict.values()
                }).sort_values('Importance', ascending=False)
                
                # Add cumulative importance
                importance_df['Cumulative Importance'] = importance_df['Importance'].cumsum()
                
                # Display table with formatting
                st.dataframe(
                    importance_df.style.format({
                        'Importance': '{:.3f}',
                        'Cumulative Importance': '{:.3f}'
                    }).bar(subset=['Importance'], color='#0083B8')
                )
                
                # Feature importance insights
                st.write("**Key Insights:**")
                top_features = importance_df.head(3)['Feature'].tolist()
                st.write(f"• Top 3 most important features: {', '.join(top_features)}")
                cumulative_80 = len(importance_df[importance_df['Cumulative Importance'] <= 0.8])
                st.write(f"• {cumulative_80} features account for 80% of the model's predictions")

        # Model Management tab
        with tabs[4]:
            st.subheader("Model Management")
            render_model_management_ui(model_manager)
            
            # Model metadata section
            if st.session_state.get('model_metadata'):
                with st.expander("Current Model Metadata", expanded=True):
                    metadata = st.session_state['model_metadata']
                    
                    st.write("**General Information**")
                    st.write(f"- Model Type: {metadata['model_type']}")
                    st.write(f"- Version: {metadata['version']}")
                    st.write(f"- Training Date: {metadata['training_date']}")
                    
                    st.write("\n**Model Parameters**")
                    for param, value in metadata['model_params'].items():
                        st.write(f"- {param}: {value}")
                    
                    st.write("\n**Performance Metrics**")
                    for metric, value in metadata['metrics'].items():
                        st.write(f"- {metric}: {value:.4f}")

if __name__ == "__main__":
    app()

================
File: pages/4_Predictions.py
================
import streamlit as st
import pandas as pd
import numpy as np
import os
import joblib
from utils.data_processor import DataProcessor, FEATURE_COLUMNS
from models.polynomial_regression import PoultryWeightPredictor
from config.settings import MODEL_SAVE_PATH

def validate_input_values(input_values: dict) -> bool:
    """Validate input values for manual prediction."""
    for key, value in input_values.items():
        if value is None or pd.isna(value):
            st.error(f"Invalid value for {key}")
            return False
    return True

def app():
    st.title("🔮 Make Predictions")
    
    # Initialize data processor
    data_processor = DataProcessor()
    
    # Sidebar - Model Selection
    st.sidebar.subheader("Model Selection")
    
    # Option to use trained model or load saved model
    model_option = st.sidebar.radio(
        "Choose Model Source",
        ["Use Currently Trained Model", "Load Saved Model"]
    )
    
    # Model loading section
    try:
        if model_option == "Use Currently Trained Model":
            if 'model' not in st.session_state:
                st.error("No trained model found! Please train a model first.")
                st.stop()
            model = st.session_state['model']
            
            # Use the data processor from training if available
            if 'data_processor' in st.session_state:
                data_processor = st.session_state['data_processor']
                print("Using data processor from training session")
        else:
            # Load saved model
            if not os.path.exists(MODEL_SAVE_PATH):
                st.error(f"Model directory not found: {MODEL_SAVE_PATH}")
                st.stop()
                
            saved_models = [f for f in os.listdir(MODEL_SAVE_PATH) if f.endswith('.joblib')]
            
            if not saved_models:
                st.error("No saved models found!")
                st.stop()
                
            selected_model = st.sidebar.selectbox("Select Saved Model", saved_models)
            try:
                model_path = os.path.join(MODEL_SAVE_PATH, selected_model)
                saved_data = joblib.load(model_path)
                
                # Check if it's a dictionary containing model and data_processor
                if isinstance(saved_data, dict):
                    model = saved_data['model']
                    data_processor = saved_data['data_processor']
                else:
                    model = saved_data
                
                if not isinstance(model, PoultryWeightPredictor):
                    st.error("Invalid model file!")
                    st.stop()
                
                st.sidebar.success(f"Model loaded successfully: {selected_model}")
                
            except Exception as e:
                st.error(f"Error loading model: {str(e)}")
                st.stop()
                
    except Exception as e:
        st.error(f"Error initializing model: {str(e)}")
        st.stop()
    
    # Main content
    st.subheader("Make New Predictions")
    
    # Input method selection
    input_method = st.radio(
        "Choose Input Method",
        ["Manual Input", "Batch Prediction (CSV)"]
    )
    
    if input_method == "Manual Input":
        st.markdown("### Enter Feature Values")
        
        try:
            # Create input fields for each feature
            input_values = {}
            col1, col2 = st.columns(2)
            
            with col1:
                input_values['Int Temp'] = st.number_input(
                    "Internal Temperature (°C)",
                    min_value=0.0,
                    max_value=50.0,
                    value=25.0,
                    step=0.1,
                    help="Temperature inside the poultry house"
                )
                input_values['Int Humidity'] = st.number_input(
                    "Internal Humidity (%)",
                    min_value=0.0,
                    max_value=100.0,
                    value=60.0,
                    step=1.0,
                    help="Humidity level inside the poultry house"
                )
                input_values['Air Temp'] = st.number_input(
                    "Air Temperature (°C)",
                    min_value=0.0,
                    max_value=50.0,
                    value=23.0,
                    step=0.1,
                    help="Outside air temperature"
                )
            
            with col2:
                input_values['Wind Speed'] = st.number_input(
                    "Wind Speed (m/s)",
                    min_value=0.0,
                    max_value=20.0,
                    value=2.0,
                    step=0.1,
                    help="Wind speed measurement"
                )
                input_values['Feed Intake'] = st.number_input(
                    "Feed Intake (g)",
                    min_value=0.0,
                    value=100.0,
                    step=1.0,
                    help="Amount of feed consumed"
                )
            
            if st.button("Predict"):
                try:
                    # Validate input values
                    if not validate_input_values(input_values):
                        st.stop()
                    
                    # Create DataFrame from input
                    input_df = pd.DataFrame([input_values])
                    
                    # Show input data
                    st.write("Input Data:")
                    st.dataframe(input_df)
                    
                    # Scale features directly (no need for preprocessing for single prediction)
                    scaled_features = data_processor.scale_features(input_df[FEATURE_COLUMNS])
                    
                    # Make prediction
                    prediction = model.predict(scaled_features)
                    
                    # Display prediction
                    st.success(f"Predicted Weight: {prediction[0]:.2f} g")
                    
                    # Add to prediction history
                    if 'prediction_history' not in st.session_state:
                        st.session_state['prediction_history'] = []
                    
                    st.session_state['prediction_history'].append({
                        **input_values,
                        'Predicted Weight': prediction[0]
                    })
                    
                except Exception as e:
                    st.error(f"Error making prediction: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
        
        except Exception as e:
            st.error(f"Error setting up input fields: {str(e)}")
    
    else:  # Batch Prediction
        st.markdown("### Upload Data for Batch Prediction")
        
        # Show sample format
        st.info("Your CSV file should have these columns: " + ", ".join(FEATURE_COLUMNS))
        
        # Sample data
        sample_df = pd.DataFrame({
            'Int Temp': [25.0, 26.0],
            'Int Humidity': [60.0, 65.0],
            'Air Temp': [23.0, 24.0],
            'Wind Speed': [2.0, 2.5],
            'Feed Intake': [100.0, 110.0]
        })
        
        # Show and download sample template
        st.write("Sample Format:")
        st.dataframe(sample_df)
        st.download_button(
            "Download Sample Template",
            sample_df.to_csv(index=False),
            "sample_template.csv",
            "text/csv"
        )
        
        uploaded_file = st.file_uploader(
            "Upload CSV file",
            type=['csv'],
            help="Upload a CSV file containing the required features"
        )
        
        if uploaded_file is not None:
            try:
                # Read data
                prediction_df = pd.read_csv(uploaded_file)
                
                # Show raw data
                st.subheader("Input Data Preview")
                st.dataframe(prediction_df.head())
                
                # Validate columns
                is_valid, missing_cols = data_processor.validate_columns(prediction_df)
                
                if not is_valid:
                    st.error(f"Missing required columns: {', '.join(missing_cols)}")
                    st.write("Required columns:", FEATURE_COLUMNS)
                    st.stop()
                
                # Scale features
                scaled_features = data_processor.scale_features(prediction_df[FEATURE_COLUMNS])
                
                # Make predictions
                predictions = model.predict(scaled_features)
                
                # Create results DataFrame
                results_df = prediction_df.copy()
                results_df['Predicted_Weight'] = predictions
                
                # Display results
                st.subheader("Prediction Results")
                st.dataframe(results_df)
                
                # Show statistics
                st.subheader("Prediction Statistics")
                stats = results_df['Predicted_Weight'].describe()
                st.write(stats)
                
                # Download results
                st.download_button(
                    label="Download Predictions",
                    data=results_df.to_csv(index=False),
                    file_name="predictions.csv",
                    mime="text/csv"
                )
                
            except Exception as e:
                st.error(f"Error processing file: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
    
    # Show prediction history
    if 'prediction_history' in st.session_state and st.session_state['prediction_history']:
        st.subheader("Prediction History")
        history_df = pd.DataFrame(st.session_state['prediction_history'])
        st.dataframe(history_df)
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("Clear History"):
                st.session_state['prediction_history'] = []
                st.experimental_rerun()
        with col2:
            st.download_button(
                label="Download History",
                data=history_df.to_csv(index=False),
                file_name="prediction_history.csv",
                mime="text/csv"
            )

if __name__ == "__main__":
    app()

================
File: pages/5_Model_Comparison.py
================
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from utils.model_comparison import ModelComparison
from utils.model_manager import ModelManager
from utils.model_management_ui import render_model_management_ui
from models.model_factory import ModelFactory

def validate_model_data(metadata: dict, model_id: str) -> bool:
    """Validate that required model data is present."""
    required_fields = ['metrics', 'predictions', 'actual']
    missing_fields = [field for field in required_fields if field not in metadata]
    
    if missing_fields:
        st.warning(f"Model {model_id} is missing required data: {', '.join(missing_fields)}")
        return False
    
    # Check for None values
    if metadata['predictions'] is None or metadata['actual'] is None:
        st.warning(f"Model {model_id} has no prediction data. Re-train the model to enable comparison.")
        return False
        
    return True

def app():
    st.title("📊 Model Comparison")
    
    # Initialize model manager
    model_manager = ModelManager()
    
    # Get list of saved models
    models_df = model_manager.list_models()
    
    if models_df.empty:
        st.warning("No saved models available for comparison. Please train some models first!")
        return
        
    # Model selection section
    st.subheader("Select Models for Comparison")
    
    # Multi-select for models
    selected_models = st.multiselect(
        "Choose models to compare",
        options=models_df['model_id'].tolist(),
        format_func=lambda x: models_df[models_df['model_id'] == x]['model_name'].iloc[0],
        help="Select two or more models to compare"
    )
    
    if len(selected_models) < 2:
        st.warning("Please select at least two models to enable comparison!")
        return
    
    # Initialize model comparison
    comparison = ModelComparison()
    
    # Try loading the selected models
    valid_models = []
    loaded_data = False
    
    try:
        for model_id in selected_models:
            model, metadata = model_manager.load_model(model_id)
            
            # Validate model data
            if validate_model_data(metadata, model_id):
                # Map feature importance to actual feature names
                feature_importance = metadata.get('feature_importance')
                if feature_importance:
                    # Define the feature name mapping
                    feature_names = ['Int Temp', 'Int Humidity', 'Air Temp', 'Wind Speed', 'Feed Intake']
                    
                    # Create new dictionary with proper feature names
                    mapped_importance = {}
                    for i, (_, value) in enumerate(feature_importance.items()):
                        if i < len(feature_names):
                            mapped_importance[feature_names[i]] = value
                    
                    feature_importance = mapped_importance

                comparison.add_model_results(
                    model_name=metadata['model_name'],
                    metrics=metadata['metrics'],
                    predictions=np.array(metadata['predictions']),
                    actual=np.array(metadata['actual']),
                    feature_importance=feature_importance,
                    metadata=metadata
                )
                valid_models.append(model_id)
                loaded_data = True
        
        if not valid_models:
            st.error("No valid models available for comparison. Please ensure models have prediction data.")
            return
            
        if len(valid_models) < 2:
            st.error("At least two valid models with prediction data are required for comparison.")
            return
        
        st.success(f"Successfully loaded {len(valid_models)} models for comparison")
        
    except Exception as e:
        st.error(f"Error loading models: {str(e)}")
        st.exception(e)
        return

    if loaded_data:
        # Create tabs for different comparisons
        tabs = st.tabs([
            "Overview",
            "Metrics Comparison",
            "Predictions Comparison",
            "Error Analysis",
            "Feature Importance",
            "Model Details"
        ])
        
        # Overview tab
        with tabs[0]:
            st.markdown("## 📊 Model Comparison Overview")
            
            # Get insights
            insights = comparison.generate_comparison_insights()
            
            # Create two columns for the overview
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("### 🏆 Best Performing Model")
                best_model = insights['overall_best']['model']
                metrics = comparison.get_metrics_comparison()[best_model]
                
                # Display best model metrics in a nice format
                st.markdown(
                    f"""
                    <div style='padding: 10px; border-radius: 5px; background-color: #f0f8ff;'>
                        <h4>{best_model}</h4>
                        <p>Key Metrics:</p>
                        <ul>
                            <li>R² Score: {metrics['r2']:.4f}</li>
                            <li>RMSE: {metrics['rmse']:.4f}</li>
                            <li>MAE: {metrics['mae']:.4f}</li>
                        </ul>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
            
            with col2:
                st.markdown("### 📈 Models Count")
                st.markdown(
                    f"""
                    <div style='padding: 10px; border-radius: 5px; background-color: #f0fff0;'>
                        <h2 style='text-align: center;'>{len(selected_models)}</h2>
                        <p style='text-align: center;'>Models Compared</p>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
            
            # Model Comparison Matrix
            st.markdown("### 📊 Model Performance Matrix")
            metrics_df = comparison.get_metrics_comparison()
            
            # Create a more visually appealing metrics table
            styled_metrics = metrics_df.style.format("{:.4f}").apply(
                lambda x: ['background-color: #90EE90' if (
                    (x.name == 'r2' and v == x.max()) or
                    (x.name in ['mse', 'rmse', 'mae', 'mape'] and v == x.min())
                ) else '' for v in x],
                axis=1
            )
            st.dataframe(styled_metrics, use_container_width=True)
            
            # Quick Insights Section
            st.markdown("### 🔍 Quick Insights")
            
            # Create columns for different insights
            insight_cols = st.columns(3)
            
            with insight_cols[0]:
                st.markdown("#### 🎯 Model Strengths")
                for model, patterns in insights['prediction_patterns'].items():
                    if patterns['reliability'] == 'High':
                        st.markdown(f"- **{model}**: {patterns['bias_tendency']} with high reliability")
            
            with insight_cols[1]:
                st.markdown("#### 🌟 Feature Importance")
                if 'feature_importance' in insights:
                    top_features = list(insights['feature_importance'].get('common_important_features', {}).keys())[:3]
                    if top_features:
                        st.markdown("Top influential features:")
                        for feature in top_features:
                            st.markdown(f"- {feature}")
            
            with insight_cols[2]:
                st.markdown("#### 💡 Recommendations")
                recs = insights.get('recommendations', [])[:3]  # Get top 3 recommendations
                for rec in recs:
                    st.markdown(f"- {rec}")
            
            # Performance Distribution
            st.markdown("### 📉 Performance Distribution")
            col1, col2 = st.columns(2)
            
            with col1:
                # R² Score Distribution
                r2_scores = metrics_df.loc['r2']
                fig_r2 = go.Figure(data=[
                    go.Bar(
                        x=[comparison._shorten_model_name(x) for x in r2_scores.index],
                        y=r2_scores.values,
                        marker_color='lightblue'
                    )
                ])
                fig_r2.update_layout(
                    title='R² Score by Model',
                    xaxis_title='Models',
                    yaxis_title='R² Score',
                    xaxis_tickangle=45
                )
                st.plotly_chart(fig_r2, use_container_width=True)
            
            with col2:
                # RMSE Distribution
                rmse_scores = metrics_df.loc['rmse']
                fig_rmse = go.Figure(data=[
                    go.Bar(
                        x=[comparison._shorten_model_name(x) for x in rmse_scores.index],
                        y=rmse_scores.values,
                        marker_color='lightgreen'
                    )
                ])
                fig_rmse.update_layout(
                    title='RMSE by Model',
                    xaxis_title='Models',
                    yaxis_title='RMSE',
                    xaxis_tickangle=45
                )
                st.plotly_chart(fig_rmse, use_container_width=True)
        
        # Metrics Comparison tab
        with tabs[1]:
            st.subheader("Detailed Metrics Comparison")
            
            # Display metrics table
            st.markdown("### Performance Metrics Table")
            formatted_metrics = metrics_df.style.format("{:.4f}").apply(
                lambda x: ['background-color: #e6ffe6' if (
                    (x.name == 'r2' and v == x.max()) or
                    (x.name in ['mse', 'rmse', 'mae', 'mape'] and v == x.min())
                ) else '' for v in x],
                axis=1
            )
            st.dataframe(formatted_metrics, use_container_width=True)
            
            # Metric visualization
            st.markdown("### Metric Visualization")
            metric_to_plot = st.selectbox(
                "Select metric to visualize",
                options=metrics_df.index,
                format_func=lambda x: x.upper()
            )
            
            metrics_plot = comparison.plot_metrics_comparison(metric_to_plot)
            st.plotly_chart(metrics_plot, use_container_width=True)
        
        # Predictions Comparison tab
        with tabs[2]:
            st.subheader("Predictions Analysis")
            
            # Plot predictions comparison
            st.markdown("### Actual vs Predicted Values")
            pred_plot = comparison.plot_prediction_comparison()
            st.plotly_chart(pred_plot, use_container_width=True)
            
            # Show prediction statistics
            st.markdown("### Prediction Statistics")
            stats_df = comparison.get_prediction_stats()
            
            formatted_stats = stats_df.style.format("{:.2f}").apply(
                lambda x: ['background-color: #e6ffe6' if v == x.min() else '' for v in x],
                axis=1
            )
            st.dataframe(formatted_stats, use_container_width=True)
        
        # Error Analysis tab
        with tabs[3]:
            st.subheader("Error Analysis")
            
            error_plot = comparison.plot_error_analysis()
            st.plotly_chart(error_plot, use_container_width=True)
            
            with st.expander("Detailed Error Statistics"):
                error_stats = comparison.get_prediction_stats()
                st.dataframe(
                    error_stats.style.format("{:.4f}")
                    .background_gradient(cmap='RdYlGn_r', subset=['Mean Error', 'Max Error']),
                    use_container_width=True
                )

        # Feature Importance tab
        with tabs[4]:
            st.markdown("## 🎯 Feature Importance Analysis")
            
            importance_plot = comparison.plot_feature_importance_comparison()
            if importance_plot is not None:
                # Add explanation of the heatmap
                st.markdown("""
                ### 📊 Feature Importance Heatmap
                The heatmap below shows the relative importance of each feature across different models.
                - **Darker colors** indicate higher importance
                - **Lighter colors** indicate lower importance
                - Compare across rows to see which features are consistently important
                """)
                
                st.plotly_chart(importance_plot, use_container_width=True)
                
                # Get common important features
                common_features = comparison.get_common_important_features(top_n=3)
                
                # Create two columns for the analysis
                col1, col2 = st.columns([3, 2])
                
                with col1:
                    st.markdown("### 🔑 Key Features Analysis")
                    st.markdown("##### Most Important Features Across Models:")
                    
                    for rank, (feature, models) in enumerate(common_features.items(), 1):
                        # Create a colored box for each feature with its importance info
                        st.markdown(f"""
                        <div style='padding: 10px; margin: 5px 0; border-radius: 5px; background-color: {"#f0f8ff" if rank % 2 == 0 else "#f5f5f5"}'>
                            <h4 style='margin: 0; color: #1e88e5;'>#{rank} {feature}</h4>
                            <p style='margin: 5px 0;'>Important in {len(models)} models</p>
                            <p style='margin: 0; font-size: 0.9em; color: #666;'>
                                Models: {', '.join([comparison._shorten_model_name(m) for m in models])}
                            </p>
                        </div>
                        """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown("### 📈 Feature Consistency")
                    total_models = len(comparison.results)
                    
                    # Calculate consistency scores
                    for feature, models in common_features.items():
                        consistency = (len(models) / total_models) * 100
                        color = ('green' if consistency >= 75 else 
                                'orange' if consistency >= 50 else 'red')
                        
                        st.markdown(f"""
                        <div style='padding: 8px; margin: 5px 0; border-radius: 5px; border-left: 4px solid {color};'>
                            <p style='margin: 0;'><strong>{feature}</strong></p>
                            <p style='margin: 0; color: {color};'>{consistency:.1f}% consistency</p>
                        </div>
                        """, unsafe_allow_html=True)
                
                # Add feature importance patterns
                st.markdown("### 🔍 Feature Importance Patterns")
                col1, col2 = st.columns(2)
                
                with col1:
                    # Most stable features
                    st.markdown("#### Most Stable Features")
                    stable_features = comparison.get_stable_features()
                    if stable_features:
                        for feature, stability_score in stable_features.items():
                            st.markdown(f"""
                            <div style='padding: 5px; margin: 2px 0;'>
                                <strong>{feature}</strong>: {stability_score:.2f} stability score
                            </div>
                            """, unsafe_allow_html=True)
                
                with col2:
                    # Feature importance recommendations
                    st.markdown("#### Recommendations")
                    st.markdown("""
                    - Focus on consistently important features
                    - Consider feature engineering for top features
                    - Monitor less important features for potential removal
                    """)
            else:
                st.info("Feature importance comparison not available for these models. This might be because the selected models don't provide feature importance information.")
                
            # Add download button for feature importance data
            if importance_plot is not None:
                st.markdown("### 📥 Export Feature Importance Data")
                # Get feature importance data
                importance_data = {}
                for model_name, result in comparison.results.items():
                    if result['feature_importance'] is not None:
                        importance_data[comparison._shorten_model_name(model_name)] = result['feature_importance']
                
                if importance_data:
                    importance_df = pd.DataFrame(importance_data)
                    csv = importance_df.to_csv(index=True)
                    st.download_button(
                        label="Download Feature Importance Data",
                        data=csv,
                        file_name="feature_importance.csv",
                        mime="text/csv"
                    )                

        
        # Model Details tab
        with tabs[5]:
            st.subheader("Model Details")
            
            for model_id in valid_models:
                metadata = model_manager.metadata[model_id]
                with st.expander(f"Model: {metadata.get('model_name', 'Unknown Model')}"):
                    st.write("**General Information**")
                    st.write(f"- Model Type: {metadata.get('model_type', 'Not specified')}")
                    st.write(f"- Version: {metadata.get('version', 'Not specified')}")
                    if 'training_date' in metadata:
                        st.write(f"- Training Date: {metadata['training_date']}")
                    
                    if 'model_params' in metadata:
                        st.write("\n**Model Parameters**")
                        for param, value in metadata['model_params'].items():
                            st.write(f"- {param}: {value}")
                    
                    if 'metrics' in metadata:
                        st.write("\n**Performance Metrics**")
                        for metric, value in metadata['metrics'].items():
                            if isinstance(value, (int, float)):
                                st.write(f"- {metric}: {value:.4f}")
                            else:
                                st.write(f"- {metric}: {value}")
                    
                    if 'data_characteristics' in metadata:
                        st.write("\n**Data Characteristics**")
                        chars = metadata['data_characteristics']
                        if 'total_samples' in chars:
                            st.write(f"- Total Samples: {chars['total_samples']}")
                        if 'features' in chars:
                            st.write(f"- Number of Features: {chars['features']}")
                        if 'data_size' in chars:
                            st.write(f"- Data Size: {chars['data_size']}")
                        if 'has_outliers' in chars:
                            st.write(f"- Contains Outliers: {chars['has_outliers']}")
                    
                    if 'training_config' in metadata:
                        st.write("\n**Training Configuration**")
                        config = metadata['training_config']
                        if 'test_size' in config:
                            st.write(f"- Test Size: {config['test_size']}")
                        if 'cross_validation' in config:
                            st.write(f"- Cross Validation: {config['cross_validation']}")
                        if config.get('cv_folds'):
                            st.write(f"- CV Folds: {config['cv_folds']}")
                        if 'parameters' in config:
                            st.write("- Model Parameters:")
                            for param, value in config['parameters'].items():
                                st.write(f"  • {param}: {value}")
        
        # Export options
        st.sidebar.subheader("Export Options")
        if st.sidebar.button("Generate Report"):
            report = comparison.export_comparison_report()
            
            # Convert report to Excel
            output = pd.ExcelWriter('model_comparison_report.xlsx', engine='xlsxwriter')
            
            # Write each component
            report['metrics_comparison'].to_excel(output, sheet_name='Metrics')
            report['prediction_comparison'].to_excel(output, sheet_name='Predictions')
            
            # Export error statistics
            stats_df = comparison.get_prediction_stats()
            stats_df.to_excel(output, sheet_name='Error Statistics')
            
            output.close()
            
            # Offer download
            with open('model_comparison_report.xlsx', 'rb') as f:
                st.sidebar.download_button(
                    label="Download Report",
                    data=f,
                    file_name="model_comparison_report.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

if __name__ == "__main__":
    app()

================
File: pages/6_About.py
================
import streamlit as st

def app():
    st.title("ℹ️ About")
    
    st.markdown("""
    # Poultry Weight Predictor

    This application helps poultry farmers and researchers predict poultry weight based on environmental 
    and feeding data using advanced machine learning techniques and comprehensive model comparison capabilities.

    ## Features

    1. **Data Upload and Analysis**
       - Upload CSV files with poultry data
       - Automatic data validation and preprocessing
       - Interactive data visualization
       - Comprehensive statistical analysis
       - Multi-level outlier detection and analysis
       - Advanced data quality assessment

    2. **Advanced Analytics**
       - Time series analysis of weight progression
       - Feature relationship exploration
       - Multi-dimensional correlation analysis
       - Pattern detection and visualization
       - Comprehensive outlier analysis across features
       - Interactive data exploration tools
                
      3. **Intelligent Model Selection**
       - Automated model recommendations based on:
         * Dataset size and characteristics
         * Presence of outliers
         * Data complexity
         * Feature relationships
       - Detailed reasoning for each recommendation
       - Alternative model suggestions
       - Parameter optimization guidance
       - Performance expectations
       - Use case considerations

    4. **Machine Learning Models**
       - Multiple model support with intelligent selection:
         * Polynomial Regression (recommended for small, clean datasets)
         * Gradient Boosting (recommended for large, complex datasets)
         * Support Vector Regression (recommended for datasets with outliers)
         * Random Forest (recommended for balanced performance and interpretability)
       - Automated feature importance analysis
       - Model persistence and versioning
       - Cross-validation capabilities
       - Early stopping for appropriate models
       - Smart parameter suggestions

    5. **Model Training and Evaluation**
       - Interactive parameter tuning
       - Real-time performance metrics
       - Feature importance visualization
       - Advanced error analysis
       - Model saving and loading functionality
       - Comprehensive training metadata tracking

    6. **Predictions**
       - Single prediction through manual input
       - Batch predictions through CSV upload
       - Prediction history tracking
       - Confidence intervals
       - Downloadable prediction results
       - Performance monitoring
       - Prediction validation

    7. **Model Comparison**
       - Side-by-side model performance comparison
       - Comparative metrics visualization
       - Feature importance comparison across models
       - Prediction accuracy analysis
       - Detailed performance metrics:
         * Mean Squared Error (MSE)
         * Root Mean Squared Error (RMSE)
         * R² Score
         * Mean Absolute Error (MAE)
         * Mean Absolute Percentage Error (MAPE)
       - Exportable comparison reports
       - Visual performance charts
       - Best model recommendation based on data characteristics

    ## How to Use

    1. **Data Upload**: Start by uploading your CSV file in the Data Upload page
    2. **Data Analysis**: Explore and analyze your data with interactive visualizations
    3. **Model Training**: Train different models with optimized parameters
    4. **Model Comparison**: Compare models to select the best performer
    5. **Predictions**: Make predictions using your chosen model
    6. **Export Results**: Download predictions and comparison reports

    ## Data Requirements

    Your input data should contain the following features:
    - Internal Temperature (°C)
    - Internal Humidity (%)
    - Air Temperature (°C)
    - Wind Speed (m/s)
    - Feed Intake (g)
    - Weight (g) - required for training data only

    ## Model Details

    ### Polynomial Regression
    - Captures non-linear relationships
    - Good for baseline predictions
    - Highly interpretable results
    - Efficient with smaller datasets
    - Perfect for understanding basic patterns

    ### Gradient Boosting
    - Handles complex patterns
    - High prediction accuracy
    - Robust feature importance
    - Excellent for large datasets
    - Support for early stopping

    ### Support Vector Regression
    - Robust to outliers
    - Excellent generalization
    - Handles non-linear relationships
    - Kernel-based learning
    - Perfect for medium-sized datasets
                
      ### Random Forest
    - Excellent balance of performance and interpretability
    - Built-in feature importance
    - Robust to outliers and noise
    - Provides uncertainty estimates
    - No feature scaling required
    - Great for medium to large datasets            

    ### Model Comparison Capabilities
    - Automated performance metric calculation
    - Visual comparison tools
    - Feature importance analysis
    - Prediction accuracy comparison
    - Export functionality for detailed reports
    - Best model selection assistance
    - Cross-model validation

    ## Technical Details

    - Built with Streamlit for interactive web interface
    - Scikit-learn for machine learning models
    - Plotly for interactive visualizations
    - Pandas for efficient data manipulation
    - Advanced error handling and validation
    - Comprehensive model comparison framework
    - Robust data processing pipeline

    ## Data Security

    - Local data processing
    - No data storage without user consent
    - Secure model saving and loading
    - Privacy-focused design
    - Transparent data handling

    ## Support

    For support, feature requests, or bug reports, please contact:
    - Email: Bomino@mlawali.com
    - GitHub: [Project Repository](https://github.com/bomino/PoultryPredict3)

    ## Version Information

    - Current Version: 2.1.0
    - Last Updated: November 2024
    - Key Features: 
      * Multi-model support with Random Forest
      * Enhanced model comparison capabilities
      * Improved feature importance analysis
      * Advanced uncertainty estimation
      * Comprehensive cross-validation
      * Better parameter optimization
      * Enhanced error handling
    """)
    
    # Footer
    st.markdown("---")
    st.markdown("Built with ❤️ using Streamlit by Bomino")

if __name__ == "__main__":
    app()

================
File: utils/data_processor.py
================
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Define constants
REQUIRED_COLUMNS = [
    'Int Temp',
    'Int Humidity',
    'Air Temp',
    'Wind Speed',
    'Feed Intake',
    'Weight'
]

FEATURE_COLUMNS = [
    'Int Temp',
    'Int Humidity',
    'Air Temp',
    'Wind Speed',
    'Feed Intake'
]

TARGET_COLUMN = 'Weight'
RANDOM_STATE = 42

class DataProcessor:
    def __init__(self):
        """Initialize the DataProcessor with a standard scaler."""
        self.scaler = StandardScaler()
        self.is_fitted = False
    
    def validate_data(self, df: pd.DataFrame, is_training: bool = True) -> None:
        """
        Validate the input dataframe.
        
        Args:
            df (pd.DataFrame): Input dataframe to validate
            is_training (bool): Whether this is training data (requires 2+ rows) or prediction data (1+ rows)
        """
        if df is None:
            raise ValueError("DataFrame is None")
        if df.empty:
            raise ValueError("DataFrame is empty")
        if df[FEATURE_COLUMNS].isnull().any().any():
            raise ValueError("DataFrame contains null values in feature columns")
        if is_training and len(df) < 2:
            raise ValueError("Training data must contain at least 2 rows")
        if not is_training and len(df) < 1:
            raise ValueError("Prediction data must contain at least 1 row")
            
    def validate_columns(self, df: pd.DataFrame) -> tuple[bool, list]:
        """Validate if all required columns are present in the dataframe."""
        missing_cols = []
        # For prediction, we only need feature columns
        required_cols = REQUIRED_COLUMNS if 'Weight' in df.columns else FEATURE_COLUMNS
        missing_cols = [col for col in required_cols if col not in df.columns]
        return len(missing_cols) == 0, missing_cols
    
    def preprocess_data(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:
        """
        Preprocess the input dataframe.
        
        Args:
            df (pd.DataFrame): Input dataframe to preprocess
            is_training (bool): Whether this is training data or prediction data
            
        Returns:
            pd.DataFrame: Preprocessed dataframe
        """
        # Validate input
        self.validate_data(df, is_training=is_training)
        
        # Create a copy
        df = df.copy()
        
        print("\nPreprocessing Data:")
        print(f"Initial shape: {df.shape}")
        
        # Convert data types
        columns_to_process = REQUIRED_COLUMNS if is_training else FEATURE_COLUMNS
        for col in columns_to_process:
            try:
                print(f"\nProcessing column: {col}")
                # Handle string values
                if df[col].dtype == 'object':
                    df[col] = df[col].str.strip()
                # Convert to numeric
                df[col] = pd.to_numeric(df[col], errors='coerce')
                print(f"Column {col} converted successfully")
                print(f"Sample values: {df[col].head().tolist()}")
            except Exception as e:
                print(f"Error converting column {col}: {str(e)}")
                raise ValueError(f"Error converting column {col}: {str(e)}")
        
        # Remove rows with any null values
        initial_rows = len(df)
        df = df.dropna()
        rows_dropped = initial_rows - len(df)
        print(f"\nRows dropped due to null values: {rows_dropped}")
        
        # Validate after preprocessing
        if len(df) == 0:
            raise ValueError("No valid data remaining after preprocessing")
        
        # Fit the scaler on features if training
        if is_training:
            print("\nFitting scaler on features...")
            self.scaler.fit(df[FEATURE_COLUMNS])
            self.is_fitted = True
            print("Scaler fitted successfully")
        elif not self.is_fitted:
            raise ValueError("Scaler must be fitted before preprocessing prediction data")
        
        print(f"Final shape: {df.shape}")
        return df
    
    def prepare_features(self, df: pd.DataFrame, test_size: float = 0.2) -> tuple:
        """Prepare features for model training."""
        # Validate input
        self.validate_data(df, is_training=True)
        
        # Select features and target
        X = df[FEATURE_COLUMNS]
        y = df[TARGET_COLUMN]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=RANDOM_STATE
        )
        
        # Ensure scaler is fitted
        if not self.is_fitted:
            self.scaler.fit(X_train)
            self.is_fitted = True
        
        # Scale features
        X_train_scaled = self.scaler.transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def scale_features(self, X: pd.DataFrame) -> np.ndarray:
        """Scale features using the fitted scaler."""
        if not self.is_fitted:
            raise ValueError("Scaler not fitted yet. Run preprocess_data first.")
        
        if isinstance(X, pd.DataFrame):
            missing_cols = [col for col in FEATURE_COLUMNS if col not in X.columns]
            if missing_cols:
                raise ValueError(f"Missing required feature columns: {missing_cols}")
            X = X[FEATURE_COLUMNS]
        
        return self.scaler.transform(X)
    
    @staticmethod
    def calculate_statistics(df: pd.DataFrame) -> dict:
        """Calculate basic statistics for the dataset."""
        if df.empty:
            raise ValueError("Cannot calculate statistics on empty DataFrame")
            
        stats = {}
        columns = [col for col in REQUIRED_COLUMNS if col in df.columns]
        for col in columns:
            stats[col] = {
                'mean': df[col].mean(),
                'std': df[col].std(),
                'min': df[col].min(),
                'max': df[col].max(),
                'median': df[col].median()
            }
        return stats
    
    @staticmethod
    def detect_outliers(df: pd.DataFrame, column: str) -> pd.Series:
        """Detect outliers using IQR method."""
        if df.empty:
            raise ValueError("Cannot detect outliers in empty DataFrame")
            
        if column not in df.columns:
            raise ValueError(f"Column {column} not found")
            
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return (df[column] < lower_bound) | (df[column] > upper_bound)

================
File: utils/model_comparison.py
================
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

class ModelComparison:
    def __init__(self):
        """Initialize the ModelComparison class with enhanced tracking."""
        self.results = {}
        self.predictions = {}
        self.model_metadata = {}
        self.metric_descriptions = {
            'r2': 'Coefficient of determination (higher is better)',
            'mse': 'Mean squared error (lower is better)',
            'rmse': 'Root mean squared error (lower is better)',
            'mae': 'Mean absolute error (lower is better)',
            'mape': 'Mean absolute percentage error (lower is better)'
        }
    
    def _shorten_model_name(self, name: str) -> str:
        """
        Shorten model name for better display in graphs.
        Example: 'Gradient Boosting_20241126_121613' -> 'GB_121613'
        """
        # Dictionary of common model name mappings
        model_mappings = {
            'Gradient Boosting': 'GB',
            'Polynomial Regression': 'PR',
            'Support Vector': 'SVR'
        }
        
        # Split into name and timestamp
        if '_' in name:
            base_name, timestamp = name.split('_', 1)
            
            # Shorten base name if it's in our mappings
            for full_name, short_name in model_mappings.items():
                if full_name in base_name:
                    base_name = short_name
                    break
            
            # Keep only the time part of the timestamp (last 6 digits)
            if len(timestamp) >= 6:
                timestamp = timestamp[-6:]
                
            return f"{base_name}_{timestamp}"
        
        return name


    def add_model_results(self, model_name: str, metrics: Dict[str, float], 
                         predictions: np.ndarray, actual: np.ndarray,
                         feature_importance: Dict[str, float] = None,
                         metadata: Dict = None):
        """
        Add results for a model to the comparison.
        
        Args:
            model_name: Name of the model
            metrics: Dictionary of performance metrics
            predictions: Array of predicted values
            actual: Array of actual values
            feature_importance: Dictionary of feature importance scores
            metadata: Additional model metadata
        """
        # Store basic results
        self.results[model_name] = {
            'metrics': metrics,
            'feature_importance': feature_importance
        }
        
        # Store predictions
        self.predictions[model_name] = {
            'predicted': np.array(predictions),
            'actual': np.array(actual)
        }
        
        # Store metadata
        self.model_metadata[model_name] = metadata or {}
        
        # Calculate additional metrics
        errors = np.abs(actual - predictions)
        rel_errors = np.abs((actual - predictions) / actual) * 100
        
        additional_metrics = {
            'mean_error': np.mean(errors),
            'std_error': np.std(errors),
            'max_error': np.max(errors),
            'min_error': np.min(errors),
            'error_90th_percentile': np.percentile(errors, 90),
            'error_95th_percentile': np.percentile(errors, 95)
        }
        
        # Update metrics with additional ones
        self.results[model_name]['metrics'].update(additional_metrics)
    
    def get_metrics_comparison(self) -> pd.DataFrame:
        """Get a DataFrame comparing metrics across models with enhanced formatting."""
        metrics_data = {}
        for model_name, result in self.results.items():
            metrics_data[model_name] = result['metrics']
        
        metrics_df = pd.DataFrame(metrics_data).round(4)
        
        # Sort metrics in a logical order
        metric_order = [
            'r2', 'mse', 'rmse', 'mae', 'mape',
            'mean_error', 'std_error', 'max_error', 'min_error',
            'error_90th_percentile', 'error_95th_percentile'
        ]
        
        # Reorder metrics if they exist
        available_metrics = [m for m in metric_order if m in metrics_df.index]
        other_metrics = [m for m in metrics_df.index if m not in metric_order]
        ordered_metrics = available_metrics + other_metrics
        
        return metrics_df.loc[ordered_metrics]
    #############################################################

    def get_prediction_comparison(self) -> pd.DataFrame:
        """Get a comprehensive DataFrame comparing predictions across models."""
        if not self.predictions:
            return pd.DataFrame()
        
        # First, create a base DataFrame with the actual values
        first_model = list(self.predictions.keys())[0]
        base_df = pd.DataFrame({
            'Sample': range(1, len(self.predictions[first_model]['actual']) + 1),
            'Actual': self.predictions[first_model]['actual']
        })
        
        # Add predictions and comprehensive error metrics for each model
        for model_name, pred_data in self.predictions.items():
            short_name = self._shorten_model_name(model_name)
            predicted = pred_data['predicted']
            actual = pred_data['actual']
            
            # Calculate various error metrics
            abs_error = np.abs(actual - predicted)
            rel_error = np.abs((actual - predicted) / actual) * 100
            squared_error = (actual - predicted) ** 2
            
            # Add to DataFrame
            base_df[f'Predicted_{model_name}'] = predicted
            base_df[f'Absolute_Error_{model_name}'] = abs_error
            base_df[f'Relative_Error_%_{model_name}'] = rel_error
            base_df[f'Squared_Error_{model_name}'] = squared_error
            
            # Add error direction
            base_df[f'Error_Direction_{model_name}'] = np.where(
                predicted > actual, 'Over-prediction', 'Under-prediction'
            )
        
        return base_df
    
    def get_prediction_stats(self) -> pd.DataFrame:
        """Get comprehensive prediction statistics for each model."""
        stats_data = []
        
        for model_name, pred_data in self.predictions.items():
            actual = pred_data['actual']
            predicted = pred_data['predicted']
            errors = np.abs(actual - predicted)
            rel_errors = np.abs((actual - predicted) / actual) * 100
            
            stats = {
                'Model': model_name,
                'Mean Error': np.mean(errors),
                'Median Error': np.median(errors),
                'Std Error': np.std(errors),
                'Min Error': np.min(errors),
                'Max Error': np.max(errors),
                'Mean Relative Error %': np.mean(rel_errors),
                'Median Relative Error %': np.median(rel_errors),
                '90th Percentile Error': np.percentile(errors, 90),
                '95th Percentile Error': np.percentile(errors, 95),
                'Over-predictions %': np.mean(predicted > actual) * 100,
                'Under-predictions %': np.mean(predicted < actual) * 100
            }
            
            stats_data.append(stats)
        
        return pd.DataFrame(stats_data).set_index('Model')
    
    #####################################################################

    def plot_metrics_comparison(self, metric: str = 'r2') -> go.Figure:
        """Create an enhanced bar plot comparing a specific metric across models."""
        metrics_df = self.get_metrics_comparison()
        
        if metric not in metrics_df.index:
            raise ValueError(f"Metric '{metric}' not found. Available metrics: {metrics_df.index.tolist()}")
        
        # Determine if lower is better for this metric
        lower_is_better = metric.lower() in ['mse', 'rmse', 'mae', 'mape']

        # Create mapping of full names to short names
        short_names = {name: self._shorten_model_name(name) for name in metrics_df.columns}
        
        # Sort values and get shortened names
        sorted_data = metrics_df.loc[metric].sort_values(
            ascending=metric.lower() in ['mse', 'rmse', 'mae', 'mape']
        )
        sorted_short_names = [short_names[name] for name in sorted_data.index]
        
        # Create color scale based on values
        colors = [
            'rgb(50,205,50)' if i == 0 else  # Best performer
            'rgb(255,165,0)' if i == len(sorted_data) - 1 else  # Worst performer
            'rgb(30,144,255)'  # Others
            for i in range(len(sorted_data))
        ]
        
        fig = go.Figure(data=[
            go.Bar(
                x=sorted_data.index,
                y=sorted_data.values,
                text=sorted_data.round(4),
                textposition='auto',
                marker_color=colors
            )
        ])
        
        # Add reference line for best score if appropriate
        if metric == 'r2':
            fig.add_hline(y=1.0, line_dash="dash", line_color="gray",
                         annotation_text="Perfect Score (R²=1)")
        
        fig.update_layout(
            title=f'{metric.upper()} Score Comparison<br><sup>{self.metric_descriptions.get(metric, "")}</sup>',
            xaxis_title='Models',
            yaxis_title=f'{metric.upper()} Score',
            template='plotly_white',
            showlegend=False,
            xaxis_tickangle=45  # Angle labels for better readability
        )
        
        return fig
    
    def plot_prediction_comparison(self) -> go.Figure:
        """Create an enhanced scatter plot comparing predictions."""
        if not self.predictions:
            return go.Figure()
        
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=('Predictions Comparison', 'Error Distribution'),
            vertical_spacing=0.2,
            row_heights=[0.7, 0.3]
        )

        # Get shortened names
        short_names = {name: self._shorten_model_name(name) 
                      for name in self.predictions.keys()}
        

        # Find global min and max
        all_actuals = np.concatenate([pred_data['actual'] for pred_data in self.predictions.values()])
        all_predictions = np.concatenate([pred_data['predicted'] for pred_data in self.predictions.values()])
        min_val = min(np.min(all_actuals), np.min(all_predictions))
        max_val = max(np.max(all_actuals), np.max(all_predictions))
        
        # Add perfect prediction line
        fig.add_trace(
            go.Scatter(
                x=[min_val, max_val],
                y=[min_val, max_val],
                mode='lines',
                name='Perfect Prediction',
                line=dict(color='black', dash='dash')
            ),
            row=1, col=1
        )
        
        # Add model predictions
        colors = px.colors.qualitative.Set3
        for i, (model_name, pred_data) in enumerate(self.predictions.items()):
            short_name = short_names[model_name]

            # Scatter plot
            fig.add_trace(
                go.Scatter(
                    x=pred_data['actual'],
                    y=pred_data['predicted'],
                    mode='markers',
                    name=short_name,
                    marker=dict(
                        size=8,
                        color=colors[i % len(colors)]
                    )
                ),
                row=1, col=1
            )
            
            # Error distribution
            errors = pred_data['predicted'] - pred_data['actual']
            fig.add_trace(
                go.Histogram(
                    x=errors,
                    name=f"{short_name} Errors",
                    nbinsx=30,
                    opacity=0.7,
                    marker_color=colors[i % len(colors)]
                ),
                row=2, col=1
            )
        
        fig.update_layout(
            height=800,
            title='Prediction Analysis',
            template='plotly_white'
        )
        
        # Update axes labels
        fig.update_xaxes(title_text="Actual Values", row=1, col=1)
        fig.update_yaxes(title_text="Predicted Values", row=1, col=1)
        fig.update_xaxes(title_text="Prediction Error", row=2, col=1)
        fig.update_yaxes(title_text="Count", row=2, col=1)
        
        return fig
    
    def plot_feature_importance_comparison(self) -> Optional[go.Figure]:
        """Create a heatmap comparing feature importance across models."""
        importance_data = {}
        for model_name, result in self.results.items():
            if result['feature_importance'] is not None:
                importance_data[self._shorten_model_name(model_name)] = result['feature_importance']
        
        if not importance_data:
            return None
        
        # Create DataFrame and sort features by average importance
        importance_df = pd.DataFrame(importance_data)
        avg_importance = importance_df.mean(axis=1)
        importance_df = importance_df.loc[avg_importance.sort_values(ascending=False).index]
        
        fig = go.Figure(data=go.Heatmap(
            z=importance_df.values,
            x=importance_df.columns,
            y=importance_df.index,
            colorscale='RdBu',
            text=np.round(importance_df.values, 4),
            texttemplate='%{text}',
            textfont={"size": 10},
            hoverongaps=False
        ))
        
        fig.update_layout(
            title='Feature Importance Comparison<br><sup>Darker colors indicate higher importance</sup>',
            xaxis_title='Models',
            yaxis_title='Features',
            template='plotly_white',
            height=max(400, len(importance_df) * 30),  # Dynamic height based on number of features
            xaxis_tickangle=45  # Angle model names for better readability
        )
        
        return fig
    ##################################################################

    def plot_error_analysis(self) -> go.Figure:
        """Create a comprehensive error analysis visualization."""
        if not self.predictions:
            return go.Figure()
        
        stats_df = self.get_prediction_stats()
        
        # Create mapping of full names to short names
        short_names = {model_name: self._shorten_model_name(model_name) 
                      for model_name in stats_df.index}
        
        # Create new index with shortened names
        stats_df.index = [short_names[name] for name in stats_df.index]
        
        # Create subplots for different error metrics
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'Mean Error by Model',
                'Error Distribution',
                'Error Ranges',
                'Relative Error %'
            ),
            vertical_spacing=0.2,
            horizontal_spacing=0.15
        )
        
        colors = px.colors.qualitative.Set3
        
        # Mean Error Bar Plot
        fig.add_trace(
            go.Bar(
                x=stats_df.index,
                y=stats_df['Mean Error'],
                marker_color=colors,
                name='Mean Error'
            ),
            row=1, col=1
        )
        
        # Error Distribution (Box Plot)
        for i, (model_name, short_name) in enumerate(short_names.items()):
            pred_data = self.predictions[model_name]
            errors = np.abs(pred_data['predicted'] - pred_data['actual'])
            
            fig.add_trace(
                go.Box(
                    y=errors,
                    name=short_name,
                    marker_color=colors[i % len(colors)]
                ),
                row=1, col=2
            )
        
        # Error Ranges
        for i, short_name in enumerate(stats_df.index):
            fig.add_trace(
                go.Scatter(
                    x=[stats_df.loc[short_name, 'Min Error'],
                       stats_df.loc[short_name, 'Mean Error'],
                       stats_df.loc[short_name, 'Max Error']],
                    y=[short_name] * 3,
                    mode='lines+markers',
                    name=short_name,
                    marker_color=colors[i % len(colors)]
                ),
                row=2, col=1
            )
        
        # Relative Error
        fig.add_trace(
            go.Bar(
                x=stats_df.index,
                y=stats_df['Mean Relative Error %'],
                marker_color=colors,
                name='Mean Relative Error %'
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            height=800,
            title='Comprehensive Error Analysis',
            showlegend=False,
            template='plotly_white'
        )
        
        # Update layout for better readability
        fig.update_xaxes(tickangle=45)  # Angle the labels for better fit
        fig.update_layout(margin=dict(l=50, r=50, t=100, b=50))  # Adjust margins
        
        return fig

    ##################################################################

    def generate_comparison_insights(self) -> Dict[str, Any]:
        """Generate comprehensive insights from the model comparison."""
        insights = {
            'overall_best': {},
            'metric_analysis': {},
            'prediction_patterns': {},
            'feature_importance': {},
            'recommendations': []
        }
        
        # Overall best model analysis
        metrics_df = self.get_metrics_comparison()
        model_scores = {}
        
        for model in metrics_df.columns:
            score = 0
            for metric in metrics_df.index:
                if metric == 'r2':
                    score += (metrics_df.loc[metric, model] == metrics_df.loc[metric].max()) * 2
                elif metric in ['mse', 'rmse', 'mae', 'mape']:
                    score += (metrics_df.loc[metric, model] == metrics_df.loc[metric].min())
            model_scores[model] = score
        
        best_model = max(model_scores.items(), key=lambda x: x[1])[0]
        insights['overall_best'] = {
            'model': best_model,
            'strengths': [
                metric for metric in metrics_df.index
                if (metric == 'r2' and metrics_df.loc[metric, best_model] == metrics_df.loc[metric].max()) or
                (metric in ['mse', 'rmse', 'mae', 'mape'] and metrics_df.loc[metric, best_model] == metrics_df.loc[metric].min())
            ]
        }
        
        # Metric analysis
        for metric in metrics_df.index:
            best_model = self.get_best_model(metric)
            worst_model = self.get_worst_model(metric)
            improvement = abs(
                metrics_df.loc[metric, best_model] - metrics_df.loc[metric, worst_model]
            ) / abs(metrics_df.loc[metric, worst_model]) * 100
            
            insights['metric_analysis'][metric] = {
                'best_model': best_model,
                'worst_model': worst_model,
                'improvement_percentage': improvement,
                'significance': 'High' if improvement > 20 else 'Medium' if improvement > 10 else 'Low'
            }
        
        # Prediction pattern analysis
        prediction_stats = self.get_prediction_stats()
        for model in prediction_stats.index:
            insights['prediction_patterns'][model] = {
                'bias_tendency': 'Over-prediction' if prediction_stats.loc[model, 'Over-predictions %'] > 55 else
                                'Under-prediction' if prediction_stats.loc[model, 'Under-predictions %'] > 55 else
                                'Balanced',
                'error_distribution': 'Consistent' if prediction_stats.loc[model, 'Std Error'] < 
                                    prediction_stats['Std Error'].mean() else 'Variable',
                'reliability': 'High' if prediction_stats.loc[model, 'Mean Relative Error %'] < 
                              prediction_stats['Mean Relative Error %'].mean() else 'Medium'
            }
        
        # Feature importance analysis
        if any(result['feature_importance'] is not None for result in self.results.values()):
            common_features = self.get_common_important_features()
            insights['feature_importance'] = {
                'common_important_features': common_features,
                'consistency': 'High' if len(common_features) >= 3 else 'Medium' if len(common_features) >= 2 else 'Low'
            }
        
        # Generate recommendations
        insights['recommendations'] = self._generate_recommendations(insights)
        
        return insights
    
    def get_stable_features(self) -> Dict[str, float]:
        """
        Get features that have stable importance across models.
        Returns a dictionary of features and their stability scores.
        """
        importance_data = {}
        for model_name, result in self.results.items():
            if result['feature_importance'] is not None:
                importance_data[model_name] = result['feature_importance']
        
        if not importance_data:
            return {}
        
        # Create DataFrame of feature importance
        importance_df = pd.DataFrame(importance_data)
        
        # Calculate stability score for each feature
        stability_scores = {}
        for feature in importance_df.index:
            values = importance_df.loc[feature]
            # Calculate coefficient of variation (lower means more stable)
            cv = values.std() / values.mean() if values.mean() != 0 else float('inf')
            # Convert to stability score (1 - normalized cv), higher is better
            stability_score = 1 - (cv / (1 + cv))
            stability_scores[feature] = stability_score
        
        # Sort by stability score and return top features
        return dict(sorted(stability_scores.items(), key=lambda x: x[1], reverse=True))

    def get_common_important_features(self, top_n: int = 3) -> Dict[str, List[str]]:
        """Get features that are consistently important across models."""
        important_features = {}
        
        for model_name, result in self.results.items():
            if result['feature_importance'] is not None:
                # Get top N features for this model
                top_features = sorted(
                    result['feature_importance'].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:top_n]
                
                for feature, _ in top_features:
                    if feature not in important_features:
                        important_features[feature] = []
                    important_features[feature].append(model_name)
        
        return dict(sorted(
            important_features.items(),
            key=lambda x: len(x[1]),
            reverse=True
        ))

    def get_best_model(self, metric: str = 'r2') -> str:
        """Get the best performing model for a specific metric."""
        metrics_df = self.get_metrics_comparison()
        if metric not in metrics_df.index:
            raise ValueError(f"Metric '{metric}' not found. Available metrics: {metrics_df.index.tolist()}")
        
        # For these metrics, lower is better
        lower_is_better = metric.lower() in ['mse', 'rmse', 'mae', 'mape']
        
        if lower_is_better:
            return metrics_df.loc[metric].idxmin()
        return metrics_df.loc[metric].idxmax()    
    
    def get_worst_model(self, metric: str = 'r2') -> str:
        """Get the name of the worst performing model based on a specific metric."""
        metrics_df = self.get_metrics_comparison()
        if metric not in metrics_df.index:
            raise ValueError(f"Metric '{metric}' not found. Available metrics: {metrics_df.index.tolist()}")
            
        # Handle metrics where lower is better
        lower_is_better = metric.lower() in ['mse', 'rmse', 'mae']
        if lower_is_better:
            return metrics_df.loc[metric].idxmax()
        return metrics_df.loc[metric].idxmin()
    
    def _generate_recommendations(self, insights: Dict) -> List[str]:
        """Generate specific recommendations based on the analysis."""
        recommendations = []
        
        # Best model recommendation
        recommendations.append(
            f"The overall best performing model is {insights['overall_best']['model']}, "
            f"showing particular strength in {', '.join(insights['overall_best']['strengths'])}."
        )
        
        # Specific use-case recommendations
        for model, patterns in insights['prediction_patterns'].items():
            if patterns['bias_tendency'] == 'Balanced' and patterns['reliability'] == 'High':
                recommendations.append(
                    f"{model} shows balanced predictions with high reliability, "
                    "making it suitable for general-purpose use."
                )
            elif patterns['bias_tendency'] == 'Over-prediction':
                recommendations.append(
                    f"{model} tends to over-predict, consider using it when "
                    "conservative estimates are preferred."
                )
            elif patterns['bias_tendency'] == 'Under-prediction':
                recommendations.append(
                    f"{model} tends to under-predict, consider using it when "
                    "aggressive estimates are acceptable."
                )
        
        # Feature importance based recommendations
        if 'feature_importance' in insights and insights['feature_importance']['consistency'] == 'High':
            top_features = list(insights['feature_importance']['common_important_features'].keys())[:3]
            recommendations.append(
                f"All models consistently identify {', '.join(top_features)} as the most "
                "important features. Consider focusing on these for future modeling."
            )
        
        return recommendations
    
    def get_model_rankings(self, metric: str = 'r2') -> Dict[str, float]:
        """Get models ranked by a specific metric."""
        metrics_df = self.get_metrics_comparison()
        if metric not in metrics_df.index:
            raise ValueError(f"Metric '{metric}' not found. Available metrics: {metrics_df.index.tolist()}")
        
        # For these metrics, lower is better
        lower_is_better = metric.lower() in ['mse', 'rmse', 'mae', 'mape']
        
        sorted_series = metrics_df.loc[metric].sort_values(ascending=lower_is_better)
        return sorted_series.to_dict()    
    
    def export_comparison_report(self) -> Dict[str, Any]:
        """Export a comprehensive comparison report with insights."""
        metrics_df = self.get_metrics_comparison()
        insights = self.generate_comparison_insights()
        
        return {
            'metrics_comparison': metrics_df,
            'prediction_comparison': self.get_prediction_comparison(),
            'prediction_stats': self.get_prediction_stats(),
            'model_rankings': {
                metric: self.get_model_rankings(metric).to_dict()
                for metric in metrics_df.index
            },
            'best_models': {
                metric: self.get_best_model(metric)
                for metric in metrics_df.index
            },
            'insights': insights
        }

================
File: utils/model_management_ui.py
================
import streamlit as st
import pandas as pd
from typing import Dict, List



def render_model_management_ui(model_manager):
    """Render the model management interface with a visually appealing design."""
    # Get list of all models
    models_df = model_manager.list_models()

    if models_df.empty:
        st.info("No saved models found. Train a model to get started!")
        return

    # Format the DataFrame for display
    display_df = models_df.copy()
    display_df['created_at'] = pd.to_datetime(display_df['created_at']).dt.strftime('%Y-%m-%d %H:%M')

    st.markdown("### **Saved Models**")
    st.markdown("---")

    # Define custom styling for the cards
    st.markdown("""
        <style>
            .card {
                background-color: #f9f9f9;
                border-radius: 8px;
                padding: 15px;
                margin-bottom: 10px;
                box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            }
            .card-header {
                font-size: 18px;
                font-weight: bold;
                margin-bottom: 5px;
                color: #4CAF50;
            }
            .card-content {
                font-size: 14px;
                margin-bottom: 10px;
            }
            .delete-button {
                background-color: #e74c3c;
                color: white;
                border: none;
                padding: 10px 15px;
                border-radius: 5px;
                font-size: 14px;
                font-weight: bold;
                cursor: pointer;
                text-align: center;
                transition: all 0.3s ease;
            }
            .delete-button:hover {
                background-color: #c0392b;
            }
        </style>
    """, unsafe_allow_html=True)

    # Render models as individual cards
    for _, row in display_df.iterrows():
        with st.container():
            st.markdown('<div class="card">', unsafe_allow_html=True)

            # Card Header
            st.markdown(
                f"""
                <div class="card-header">{row['model_name']}</div>
                """, unsafe_allow_html=True)

            # Card Content
            st.markdown(
                f"""
                <div class="card-content">
                    <b>Type:</b> {row['model_type']}<br>
                    <b>Created At:</b> {row['created_at']}
                </div>
                """, unsafe_allow_html=True)

            # Delete button (Streamlit native)
            delete_key = f"delete_{row['model_id']}"
            if st.button("Delete", key=delete_key):
                try:
                    model_manager.delete_model(row['model_id'])
                    st.success(f"Model '{row['model_name']}' deleted successfully!")
                    st.rerun()
                except Exception as e:
                    st.error(f"Error deleting model: {str(e)}")

            st.markdown('</div>', unsafe_allow_html=True)

    st.markdown("---")

####################################################################################################################################
def render_model_comparison_interpretation(model_manager, selected_models: List[str]):
    """Render comprehensive model comparison interpretation."""
    st.subheader("📊 Model Comparison Analysis")
    
    if not selected_models:
        st.info("Select models to compare to see the analysis.")
        return
    
    # Get comparison summary
    summary = model_manager.get_comparison_summary(selected_models)
    st.markdown(summary)
    
    # Detailed breakdown
    with st.expander("📈 Detailed Performance Analysis"):
        metrics_comparison = pd.DataFrame([
            model_manager.get_model_metrics(model_id)
            for model_id in selected_models
        ], index=[model_manager.metadata[model_id]['model_name'] for model_id in selected_models])
        
        st.dataframe(metrics_comparison.style.highlight_max(axis=0))
        
        # Add interpretation for each metric
        st.markdown("### Metric Interpretations")
        
        for metric in ['r2', 'mse', 'mae']:
            if metric in metrics_comparison.columns:
                best_model = metrics_comparison[metric].idxmax() if metric == 'r2' else metrics_comparison[metric].idxmin()
                st.markdown(f"**{metric.upper()}**: {best_model} shows the best performance with a value of {metrics_comparison.loc[best_model, metric]:.4f}")
    
    # Training characteristics comparison
    with st.expander("🔍 Training Characteristics"):
        characteristics = []
        for model_id in selected_models:
            meta = model_manager.metadata[model_id]
            chars = meta.get('data_characteristics', {})
            chars['model_name'] = meta['model_name']
            characteristics.append(chars)
        
        chars_df = pd.DataFrame(characteristics)
        if not chars_df.empty:
            st.dataframe(chars_df.set_index('model_name'))

================
File: utils/model_manager.py
================
import os
import json
from datetime import datetime
import joblib
from typing import Dict, List, Optional, Any
import pandas as pd
import numpy as np

class ModelManager:
    def __init__(self, models_dir: str = "models/saved_models"):
        """Initialize ModelManager with models directory."""
        self.models_dir = models_dir
        self.metadata_file = os.path.join(models_dir, "model_metadata.json")
        self._ensure_directory_exists()
        self.metadata = self._load_metadata()

    def _ensure_directory_exists(self):
        """Create models directory if it doesn't exist."""
        os.makedirs(self.models_dir, exist_ok=True)

    def _load_metadata(self) -> Dict:
        """Load model metadata from JSON file."""
        if os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        return {}

    def _convert_numpy_types(self, obj):
        """Recursively convert numpy types to Python native types."""
        if isinstance(obj, dict):
            return {key: self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, (str, int, float)):
            return obj
        elif obj is None:
            return None
        else:
            return str(obj)

    def _save_metadata(self):
        """Save model metadata to JSON file."""
        # Convert all metadata to JSON-serializable format
        serializable_metadata = self._convert_numpy_types(self.metadata)
        with open(self.metadata_file, 'w') as f:
            json.dump(serializable_metadata, f, indent=4)

    def save_model(self, model_name: str, model: Any, metadata: Dict) -> str:
        """
        Save model and its metadata.
        
        Args:
            model_name: Name of the model
            model: The model instance
            metadata: Dictionary containing model metadata
        Returns:
            model_id (str)
        """
        model_id = f"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        model_path = os.path.join(self.models_dir, f"{model_id}.joblib")
        
        # Ensure predictions and actual values are included
        if 'predictions' not in metadata or 'actual' not in metadata:
            raise ValueError("Model metadata must include 'predictions' and 'actual' values")
        
        # Convert all metadata to JSON-serializable format
        metadata = self._convert_numpy_types(metadata)
            
        # Save model file
        joblib.dump(model, model_path)
        
        # Update metadata
        self.metadata[model_id] = {
            **metadata,
            'model_path': model_path,
            'created_at': datetime.now().isoformat(),
            'model_name': model_name
        }
        
        try:
            self._save_metadata()
        except Exception as e:
            # If saving metadata fails, delete the model file
            if os.path.exists(model_path):
                os.remove(model_path)
            raise Exception(f"Failed to save metadata: {str(e)}")
        
        return model_id

    def load_model(self, model_id: str) -> tuple[Any, Dict]:
        """Load model and its metadata."""
        if model_id not in self.metadata:
            raise ValueError(f"Model {model_id} not found")
            
        model_path = self.metadata[model_id]['model_path']
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
            
        model = joblib.load(model_path)
        return model, self.metadata[model_id]

    def delete_model(self, model_id: str):
        """Delete model and its metadata."""
        if model_id not in self.metadata:
            raise ValueError(f"Model {model_id} not found")
            
        # Delete model file
        model_path = self.metadata[model_id]['model_path']
        if os.path.exists(model_path):
            try:
                os.remove(model_path)
                print(f"Deleted model file: {model_path}")
            except Exception as e:
                print(f"Failed to delete model file: {str(e)}")
        else:
            print(f"Model file not found: {model_path}")
            
        # Remove metadata
        del self.metadata[model_id]
        self._save_metadata()
        print(f"Deleted model metadata for: {model_id}")


    def list_models(self) -> pd.DataFrame:
        """Return DataFrame with model information."""
        if not self.metadata:
            return pd.DataFrame()
            
        models_list = []
        for model_id, meta in self.metadata.items():
            model_info = {
                'model_id': model_id,
                'model_name': meta['model_name'],
                'created_at': meta['created_at'],
                'model_type': meta.get('model_type', 'Unknown'),
                'performance_metrics': meta.get('metrics', {}),
                'data_characteristics': meta.get('data_characteristics', {})
            }
            models_list.append(model_info)
            
        return pd.DataFrame(models_list)

    def get_model_metrics(self, model_id: str) -> Dict:
        """Get model performance metrics."""
        if model_id not in self.metadata:
            raise ValueError(f"Model {model_id} not found")
        return self.metadata[model_id].get('metrics', {})

    def get_comparison_summary(self, model_ids: List[str]) -> str:
        """Generate natural language comparison summary."""
        if not model_ids:
            return "No models selected for comparison."
            
        summaries = []
        metrics = {}
        
        # Collect metrics for all models
        for model_id in model_ids:
            model_metrics = self.get_model_metrics(model_id)
            model_name = self.metadata[model_id]['model_name']
            metrics[model_name] = model_metrics
        
        # Find best model for each metric
        best_models = {}
        for metric in ['r2', 'mse', 'mae']:
            if metric in next(iter(metrics.values())):
                if metric == 'r2':
                    best_model = max(metrics.items(), key=lambda x: x[1].get(metric, 0))
                    best_models[metric] = best_model[0]
                else:
                    best_model = min(metrics.items(), key=lambda x: x[1].get(metric, float('inf')))
                    best_models[metric] = best_model[0]
        
        # Generate summary
        summaries.append(f"Comparing {len(model_ids)} models:")
        if 'r2' in best_models:
            summaries.append(f"• {best_models['r2']} shows the best overall performance with highest R² score")
        if 'mse' in best_models:
            summaries.append(f"• {best_models['mse']} achieves the lowest Mean Squared Error")
        if 'mae' in best_models:
            summaries.append(f"• {best_models['mae']} has the best Mean Absolute Error")
        
        return "\n".join(summaries)

================
File: utils/visualizations.py
================
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from config.settings import THEME_COLORS, PLOT_HEIGHT, PLOT_WIDTH

class Visualizer:
    @staticmethod
    def plot_correlation_matrix(df: pd.DataFrame):
        """Create a correlation matrix heatmap."""
        corr = df.corr()
        fig = px.imshow(
            corr,
            color_continuous_scale='RdBu',
            title='Feature Correlation Matrix'
        )
        fig.update_layout(
            height=PLOT_HEIGHT,
            width=PLOT_WIDTH
        )
        return fig
    
    @staticmethod
    def plot_feature_importance(feature_names: list, importance_values: list):
        """Create a feature importance bar plot."""
        fig = px.bar(
            x=importance_values,
            y=feature_names,
            orientation='h',
            title='Feature Importance',
            color=importance_values,
            color_continuous_scale='Viridis'
        )
        fig.update_layout(
            height=PLOT_HEIGHT,
            width=PLOT_WIDTH,
            yaxis_title='Features',
            xaxis_title='Importance'
        )
        return fig
    
    @staticmethod
    def plot_actual_vs_predicted(y_true: list, y_pred: list):
        """Create scatter plot of actual vs predicted values."""
        fig = go.Figure()
        
        # Add scatter plot
        fig.add_trace(go.Scatter(
            x=y_true,
            y=y_pred,
            mode='markers',
            name='Predictions',
            marker=dict(
                color=THEME_COLORS['secondary'],
                size=8
            )
        ))
        
        # Add perfect prediction line
        min_val = min(min(y_true), min(y_pred))
        max_val = max(max(y_true), max(y_pred))
        fig.add_trace(go.Scatter(
            x=[min_val, max_val],
            y=[min_val, max_val],
            mode='lines',
            name='Perfect Prediction',
            line=dict(
                color=THEME_COLORS['primary'],
                dash='dash'
            )
        ))
        
        fig.update_layout(
            title='Actual vs Predicted Values',
            xaxis_title='Actual Weight',
            yaxis_title='Predicted Weight',
            height=PLOT_HEIGHT,
            width=PLOT_WIDTH
        )
        
        return fig

    @staticmethod
    def plot_residuals(y_true: list, y_pred: list):
        """Create residuals plot with shape validation."""
        # Convert to numpy arrays and ensure same length
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        
        min_len = min(len(y_true), len(y_pred))
        y_true = y_true[:min_len]
        y_pred = y_pred[:min_len]
        
        residuals = y_true - y_pred
        fig = go.Figure()

        # Add residuals scatter plot
        fig.add_trace(go.Scatter(
            x=y_pred,
            y=residuals,
            mode='markers',
            name='Residuals',
            marker=dict(
                color=THEME_COLORS['secondary'],
                size=8
            )
        ))

        # Add reference lines
        fig.add_hline(y=0, line_dash="dash", line_color=THEME_COLORS['primary'])
        
        mean_residual = np.mean(residuals)
        fig.add_hline(
            y=mean_residual,
            line_dash="dot",
            line_color="red",
            name=f'Mean Residual ({mean_residual:.2f})'
        )

        # Add standard error bands
        std_residual = np.std(residuals)
        fig.add_hline(
            y=mean_residual + 2*std_residual,
            line_dash="dot",
            line_color="gray",
            name='+2 Std Dev'
        )
        fig.add_hline(
            y=mean_residual - 2*std_residual,
            line_dash="dot",
            line_color="gray",
            name='-2 Std Dev'
        )

        fig.update_layout(
            title='Residuals Analysis',
            xaxis_title='Predicted Values',
            yaxis_title='Residuals (Actual - Predicted)',
            height=PLOT_HEIGHT,
            width=PLOT_WIDTH,
            showlegend=True,
            hovermode='closest'
        )

        return fig
    
    @staticmethod
    def plot_weight_over_time(df: pd.DataFrame):
        """Create line plot of weight progression over time."""
        fig = px.line(
            df,
            y='Weight',
            title='Weight Progression Over Time',
            markers=True
        )
        
        fig.update_layout(
            height=PLOT_HEIGHT,
            width=PLOT_WIDTH,
            xaxis_title='Time Points',
            yaxis_title='Weight'
        )
        
        return fig
    
    @staticmethod
    def plot_feature_distribution(df: pd.DataFrame, column: str):
        """Create histogram of feature distribution."""
        fig = px.histogram(
            df,
            x=column,
            title=f'Distribution of {column}',
            color_discrete_sequence=[THEME_COLORS['secondary']]
        )
        
        fig.update_layout(
            height=PLOT_HEIGHT,
            width=PLOT_WIDTH,
            xaxis_title=column,
            yaxis_title='Count'
        )
        
        return fig
